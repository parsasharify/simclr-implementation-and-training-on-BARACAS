{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWfvoaxadN24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096381d9-eaa1-407f-aac1-df1ef96fc01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/dovahcrow/patchify.py\n",
            "  Cloning https://github.com/dovahcrow/patchify.py to /tmp/pip-req-build-xer9nka8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dovahcrow/patchify.py /tmp/pip-req-build-xer9nka8\n",
            "  Resolved https://github.com/dovahcrow/patchify.py to commit c9e7e15fc9cb30a5a64d152013de6275d053105b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from patchify==0.2.3) (1.21.6)\n",
            "Building wheels for collected packages: patchify\n",
            "  Building wheel for patchify (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patchify: filename=patchify-0.2.3-py3-none-any.whl size=6696 sha256=df881938f16c844351ffe7615329f0f04c2de2d3bdc6b3aae8d1af110a249b19\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e5p3h9hm/wheels/e6/c4/10/6debafaaa44c7538acf693636a403dc9ff5c43a9481cbdb286\n",
            "Successfully built patchify\n",
            "Installing collected packages: patchify\n",
            "Successfully installed patchify-0.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/dovahcrow/patchify.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from os import listdir\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv2.imread(os.path.join(folder,filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return images"
      ],
      "metadata": {
        "id": "BWXjxY37dPCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-dZgZ-bTaIH",
        "outputId": "c4f20a4a-229c-463c-918d-dcb73d63b368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/sthalles/SimCLR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ffycD64dXN3",
        "outputId": "6a9b6a77-f000-47c6-f2f4-39938b70826d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/sthalles/SimCLR\n",
            "  Cloning https://github.com/sthalles/SimCLR to /tmp/pip-req-build-zhfe4fgx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/sthalles/SimCLR /tmp/pip-req-build-zhfe4fgx\n",
            "  Resolved https://github.com/sthalles/SimCLR to commit 1848fc934ad844ae630e6c452300433fe99acfd9\n",
            "\u001b[31mERROR: git+https://github.com/sthalles/SimCLR does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 1_PB.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeBV42zLdZ5U",
        "outputId": "48564468-671b-4a71-cf5e-4f4e9aa0a57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  1_PB.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of 1_PB.zip or\n",
            "        1_PB.zip.zip, and cannot find 1_PB.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from patchify import patchify, unpatchify\n",
        "patches = patchify(data, (224, 224, 3), step=225)\n",
        "print(patches)\n",
        "print(len(patches))"
      ],
      "metadata": {
        "id": "6DSliwgcdaAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "987d8fd9-1bd9-4539-9395-dcf1ce048945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-044f37b01912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpatchify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpatchify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpatchify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_pic = load_images_from_folder('/content/drive/MyDrive/0_N')\n",
        "#list_of_pic2 = load_images_from_folder('/content/drive/MyDrive/3_FEA')\n",
        "#list_of_pic3 = load_images_from_folder('/content/drive/MyDrive/2_UDH')\n",
        "list_of_pic4 = load_images_from_folder('/content/drive/MyDrive/4_ADH')\n",
        "list_of_pic5 = load_images_from_folder('/content/drive/MyDrive/5_DCIS')\n",
        "list_of_pic6 = load_images_from_folder('/content/drive/MyDrive/6_IC')"
      ],
      "metadata": {
        "id": "VbDb4yP80Gc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for picture in list_of_pic:\n",
        "  patches = patchify(picture, (224, 224, 3), step=225)\n",
        "  for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        single_patch_img = patches[i, j, 0, :, :, :]\n",
        "        if not cv2.imwrite('/content/drive/MyDrive/data2/train/0_N/' + 'image_' + '_'+ str(i).zfill(2) + '_' + str(j).zfill(2) + '.png', single_patch_img):\n",
        "            raise Exception(\"Could not write the image\")\n"
      ],
      "metadata": {
        "id": "WryumekyL1F0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "fc06f8ee-0962-4410-b144-1bcd26174193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-653c7377371c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msingle_patch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data2/train/0_N/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'image_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_patch_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not write the image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for picture in list_of_pic2:\n",
        "  patches = patchify(picture, (224, 224, 3), step=225)\n",
        "  for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        single_patch_img = patches[i, j, 0, :, :, :]\n",
        "        if not cv2.imwrite('/content/drive/MyDrive/data2/train/3_FEA/' + 'image_' + '_'+ str(i).zfill(2) + '_' + str(j).zfill(2) + '.png', single_patch_img):\n",
        "            raise Exception(\"Could not write the image\")\n"
      ],
      "metadata": {
        "id": "EWnIqzrsNHcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for picture in list_of_pic3:\n",
        "  patches = patchify(picture, (224, 224, 3), step=225)\n",
        "  for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        single_patch_img = patches[i, j, 0, :, :, :]\n",
        "        if not cv2.imwrite('/content/drive/MyDrive/data2/train/2_UDH/' + 'image_' + '_'+ str(i).zfill(2) + '_' + str(j).zfill(2) + '.png', single_patch_img):\n",
        "            raise Exception(\"Could not write the image\")\n"
      ],
      "metadata": {
        "id": "v0Sd1-Je_4Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for picture in list_of_pic4:\n",
        "  patches = patchify(picture, (224, 224, 3), step=225)\n",
        "  for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        single_patch_img = patches[i, j, 0, :, :, :]\n",
        "        if not cv2.imwrite('/content/drive/MyDrive/data2/train/4_ADH/' + 'image_' + '_'+ str(i).zfill(2) + '_' + str(j).zfill(2) + '.png', single_patch_img):\n",
        "            raise Exception(\"Could not write the image\")\n"
      ],
      "metadata": {
        "id": "3U9IisZUn8jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for picture in list_of_pic5:\n",
        "  patches = patchify(picture, (224, 224, 3), step=225)\n",
        "  for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        single_patch_img = patches[i, j, 0, :, :, :]\n",
        "        if not cv2.imwrite('/content/drive/MyDrive/data2/train/5_DCIS/' + 'image_' + '_'+ str(i).zfill(2) + '_' + str(j).zfill(2) + '.png', single_patch_img):\n",
        "            raise Exception(\"Could not write the image\")\n"
      ],
      "metadata": {
        "id": "G5kse60tn9lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for picture in list_of_pic6:\n",
        "  patches = patchify(picture, (22, 224, 3), step=225)\n",
        "  for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        single_patch_img = patches[i, j, 0, :, :, :]\n",
        "        if not cv2.imwrite('/content/drive/MyDrive/data2/train/6_IC/' + 'image_' + '_'+ str(i).zfill(2) + '_' + str(j).zfill(2) + '.png', single_patch_img):\n",
        "            raise Exception(\"Could not write the image\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "VnxrTdk3n-UT",
        "outputId": "08ef627d-176b-4852-d79c-6dda0f8b7fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-29a467ea808e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msingle_patch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data2/train/6_IC/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'image_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_patch_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not write the image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def main(data_path, out_path, train_ratio):\n",
        "    #1\n",
        "    dir_paths = [child for child in Path(data_path).iterdir() if child.is_dir()]\n",
        "\n",
        "    for i, dir_path in enumerate(dir_paths):\n",
        "        #2\n",
        "        files = list(dir_path.iterdir())\n",
        "        train_len = int(len(files) * (1 - train_ratio))\n",
        "\n",
        "        #3\n",
        "        out_dir = Path(out_path).joinpath(dir_path.name)\n",
        "        if not out_dir.exists():\n",
        "            out_dir.mkdir(parents=True)\n",
        "\n",
        "        #4\n",
        "        for file_ in files[:train_len]:\n",
        "            file_.replace(out_dir.joinpath(file_.name))\n",
        "\n"
      ],
      "metadata": {
        "id": "qwAnMlxnxD8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main('/content/drive/MyDrive/data2/train', '/content/drive/MyDrive/data2/test', 0.8)"
      ],
      "metadata": {
        "id": "2gBeF8RxxKgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/spijkervet/SimCLR.git\n",
        "%cd SimCLR\n",
        "!mkdir -p logs && cd logs && wget https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar && cd ../\n",
        "!sh setup.sh || python3 -m pip install -r requirements.txt || exit 1\n",
        "!pip install  pyyaml --upgrade"
      ],
      "metadata": {
        "id": "d556eXRu0L9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721533cd-99f8-4bfc-caeb-f9092962abb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SimCLR'...\n",
            "remote: Enumerating objects: 536, done.\u001b[K\n",
            "remote: Total 536 (delta 0), reused 0 (delta 0), pack-reused 536\u001b[K\n",
            "Receiving objects: 100% (536/536), 328.95 KiB | 11.34 MiB/s, done.\n",
            "Resolving deltas: 100% (295/295), done.\n",
            "/content/SimCLR\n",
            "--2023-02-02 20:40:59--  https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/246276098/8ae3c180-64bd-11ea-91fe-0f47017fe9be?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230202T204059Z&X-Amz-Expires=300&X-Amz-Signature=22aaf7ab08d7902d4c3801af26dbda43d2ec59bb26f2671820a2fd90b748d9e6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=246276098&response-content-disposition=attachment%3B%20filename%3Dcheckpoint_100.tar&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-02-02 20:40:59--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/246276098/8ae3c180-64bd-11ea-91fe-0f47017fe9be?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230202T204059Z&X-Amz-Expires=300&X-Amz-Signature=22aaf7ab08d7902d4c3801af26dbda43d2ec59bb26f2671820a2fd90b748d9e6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=246276098&response-content-disposition=attachment%3B%20filename%3Dcheckpoint_100.tar&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111607632 (106M) [application/octet-stream]\n",
            "Saving to: ‘checkpoint_100.tar’\n",
            "\n",
            "checkpoint_100.tar  100%[===================>] 106.44M  23.9MB/s    in 5.1s    \n",
            "\n",
            "2023-02-02 20:41:05 (20.8 MB/s) - ‘checkpoint_100.tar’ saved [111607632/111607632]\n",
            "\n",
            "setup.sh: 2: conda: not found\n",
            "setup.sh: 2: conda: not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.14.1+cu116)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# whether to use a TPU or not (set in Runtime -> Change Runtime Type)\n",
        "use_tpu = False"
      ],
      "metadata": {
        "id": "b33kd8x1NbC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if use_tpu:\n",
        "  VERSION = \"20200220\" #@param [\"20200220\",\"nightly\", \"xrt==1.15.0\"]\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "metadata": {
        "id": "S45gxfrVNOor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if use_tpu:\n",
        "  # imports the torch_xla package for TPU support\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  dev = xm.xla_device()\n",
        "  print(dev)\n",
        "\n",
        "import torchvision\n",
        "import argparse\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "apex = False\n",
        "try:\n",
        "    from apex import amp\n",
        "    apex = True\n",
        "except ImportError:\n",
        "    print(\n",
        "        \"Install the apex package from https://www.github.com/nvidia/apex to use fp16 for training\"\n",
        "    )\n",
        "\n",
        "from model import save_model, load_optimizer\n",
        "from simclr import SimCLR\n",
        "from simclr.modules import get_resnet, NT_Xent\n",
        "from simclr.modules.transformations import TransformsSimCLR"
      ],
      "metadata": {
        "id": "pfR0HRLvNXgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120f0beb-a7ff-402a-dc6d-084229311979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Install the apex package from https://www.github.com/nvidia/apex to use fp16 for training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import argparse\n",
        "from utils import yaml_config_hook\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"SimCLR\")\n",
        "config = yaml_config_hook(\"./config/config.yaml\")\n",
        "for k, v in config.items():\n",
        "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
        "\n",
        "args = parser.parse_args([])\n",
        "args.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "FVItJGj-Ne6J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "09876f40-ef36-4147-c643-69316e8f9e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-87222cf57d1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml_config_hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SimCLR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### override any configuration parameters here, e.g. to adjust for use on GPUs on the Colab platform:\n",
        "args.batch_size = 32#128\n",
        "args.resnet = \"resnet50\"\n",
        "pprint(vars(args))"
      ],
      "metadata": {
        "id": "b7D2W3s3N0tw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5bae9cb-c95f-4e4c-ea0c-8aa68f8c0a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 32,\n",
            " 'dataparallel': 0,\n",
            " 'dataset': 'CIFAR10',\n",
            " 'dataset_dir': './datasets',\n",
            " 'device': device(type='cuda'),\n",
            " 'epoch_num': 100,\n",
            " 'epochs': 100,\n",
            " 'gpus': 1,\n",
            " 'image_size': 224,\n",
            " 'logistic_batch_size': 256,\n",
            " 'logistic_epochs': 500,\n",
            " 'model_path': 'save',\n",
            " 'nodes': 1,\n",
            " 'nr': 0,\n",
            " 'optimizer': 'Adam',\n",
            " 'pretrain': True,\n",
            " 'projection_dim': 64,\n",
            " 'reload': False,\n",
            " 'resnet': 'resnet50',\n",
            " 'seed': 42,\n",
            " 'start_epoch': 0,\n",
            " 'temperature': 0.5,\n",
            " 'weight_decay': 1e-06,\n",
            " 'workers': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/drive/MyDrive/data2/train'\n",
        "#validation_dir = '/content/drive/MyDrive/data/validation'\n",
        "test_dir= '/content/drive/MyDrive/data2/test'\n",
        "\n",
        "\n",
        "dirs = {'train': train_dir,\n",
        "        #'validation':  validation_dir,\n",
        "        'test' : test_dir\n",
        "       }"
      ],
      "metadata": {
        "id": "DUzSb0EoPM6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([#todo: more\n",
        "       # transforms.GaussianBlur(kernel_size=501),\n",
        "       # transforms.CenterCrop(10),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        #transforms.Resize((224, 224)),#attention\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                            [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "\n",
        "    'test':transforms.Compose([\n",
        "        #transforms.GaussianBlur(kernel_size=501),\n",
        "         #transforms.CenterCrop(10),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                            [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "kvgwDTt6Pc40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, datasets\n",
        "image_datasets = {x: datasets.ImageFolder( dirs[x],   transform=TransformsSimCLR(size=args.image_size),) for x in ['train',  'test']}\n",
        "# load the data into batches\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=args.batch_size, shuffle=True, drop_last=True,\n",
        "    num_workers=args.workers,\n",
        "                                              ) for x in ['train',  'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x])\n",
        "                              for x in ['train',  'test']}"
      ],
      "metadata": {
        "id": "Ud3dir79PkqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a1f2dd-a9db-4552-e0c1-ddd9f75fbf5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = image_datasets['train']\n",
        "\n",
        "if args.nodes > 1:\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset, num_replicas=args.world_size, rank=rank, shuffle=True\n",
        "    )\n",
        "else:\n",
        "    train_sampler = None\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=(train_sampler is None),\n",
        "    drop_last=True,\n",
        "    num_workers=args.workers,\n",
        "    sampler=train_sampler,\n",
        ")"
      ],
      "metadata": {
        "id": "CNRXR3iq7p8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_loader = dataloaders['train']"
      ],
      "metadata": {
        "id": "h6kQFVmtP0WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize ResNet\n",
        "encoder = get_resnet(args.resnet, pretrained=False)\n",
        "n_features = encoder.fc.in_features  # get dimensions of fc layer\n",
        "\n",
        "# initialize model\n",
        "model = SimCLR(encoder, args.projection_dim, n_features)\n",
        "if args.reload:\n",
        "    model_fp = os.path.join(\n",
        "        args.model_path, \"checkpoint_{}.tar\".format(args.epoch_num)\n",
        "    )\n",
        "    model.load_state_dict(torch.load(model_fp, map_location=args.device.type))\n",
        "model = model.to(args.device)\n",
        "\n",
        "# optimizer / loss\n",
        "optimizer, scheduler = load_optimizer(args, model)"
      ],
      "metadata": {
        "id": "QAi729MjOCDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5c6ff2-5dea-4060-b9b1-ed778d0570cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = NT_Xent(args.batch_size, args.temperature, world_size=1)"
      ],
      "metadata": {
        "id": "ztPwSk4-OKwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter()"
      ],
      "metadata": {
        "id": "wbiHIDiBOLeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, train_loader, model, criterion, optimizer, writer):\n",
        "    loss_epoch = 0\n",
        "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x_i = x_i.cuda(non_blocking=True)\n",
        "        x_j = x_j.cuda(non_blocking=True)\n",
        "\n",
        "        # positive pair, with encoding\n",
        "        h_i, h_j, z_i, z_j = model(x_i, x_j)\n",
        "\n",
        "        loss = criterion(z_i, z_j)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
        "\n",
        "        writer.add_scalar(\"Loss/train_epoch\", loss.item(), args.global_step)\n",
        "        loss_epoch += loss.item()\n",
        "        args.global_step += 1\n",
        "    return loss_epoch\n"
      ],
      "metadata": {
        "id": "KNV_jglvOQgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/‏\n",
        "import os\n",
        "\n",
        "args.global_step = 0\n",
        "args.current_epoch = 0\n",
        "for epoch in range(args.start_epoch, args.epochs):\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "    loss_epoch = train(args, train_loader, model, criterion, optimizer, writer)\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "    # save every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        #save_model(args, model, optimizer)\n",
        "        #torch.save(model.state_dict(), os.path.join(\"/content/drive/MyDrive/model\", 'epoch-{}.pth'.format(epoch)))\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss_epoch,\n",
        "            }, os.path.join(\"/content/drive/MyDrive/model\", 'epoch-{}.pth'.format(epoch)))\n",
        "\n",
        "    writer.add_scalar(\"Loss/train\", loss_epoch / len(train_loader), epoch)\n",
        "    writer.add_scalar(\"Misc/learning_rate\", lr, epoch)\n",
        "    print(\n",
        "        f\"Epoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\"\n",
        "    )\n",
        "    args.current_epoch += 1\n",
        "\n",
        "# end training\n",
        "#save_model(args, model, optimizer)\n",
        "#torch.save({\n",
        "#            'epoch': epoch,\n",
        "#            'model_state_dict': model.state_dict(),\n",
        "#            'optimizer_state_dict': optimizer.state_dict(),\n",
        "#            'loss': loss_epoch,\n",
        "#            }, '/content/saving.pth')\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss_epoch,\n",
        "            }, os.path.join(\"/content/drive/MyDrive/model\", 'epoch.pth'))\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(\"/content/drive/MyDrive/model\", 'model.pth'))\n"
      ],
      "metadata": {
        "id": "321xiCTNOTRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1997753c-55e7-4722-ded2-c121de928dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [0/60]\t Loss: 4.119250774383545\n",
            "Step [50/60]\t Loss: 4.1916584968566895\n",
            "Epoch [0/100]\t Loss: 4.103244062264761\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.088619232177734\n",
            "Step [50/60]\t Loss: 4.132639408111572\n",
            "Epoch [1/100]\t Loss: 4.062780471642812\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.121533393859863\n",
            "Step [50/60]\t Loss: 4.116842269897461\n",
            "Epoch [2/100]\t Loss: 4.083303888638814\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.985363245010376\n",
            "Step [50/60]\t Loss: 4.042835235595703\n",
            "Epoch [3/100]\t Loss: 4.067571798960368\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.0919084548950195\n",
            "Step [50/60]\t Loss: 4.025319576263428\n",
            "Epoch [4/100]\t Loss: 4.053043591976166\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.076090335845947\n",
            "Step [50/60]\t Loss: 4.018505096435547\n",
            "Epoch [5/100]\t Loss: 4.052698373794556\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.045604705810547\n",
            "Step [50/60]\t Loss: 4.02369499206543\n",
            "Epoch [6/100]\t Loss: 4.034478970368704\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.140396595001221\n",
            "Step [50/60]\t Loss: 4.167054653167725\n",
            "Epoch [7/100]\t Loss: 4.056857144832611\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.9285850524902344\n",
            "Step [50/60]\t Loss: 4.0468525886535645\n",
            "Epoch [8/100]\t Loss: 4.009725014368693\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.017172336578369\n",
            "Step [50/60]\t Loss: 4.123045444488525\n",
            "Epoch [9/100]\t Loss: 4.081892883777618\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.11460018157959\n",
            "Step [50/60]\t Loss: 4.078523635864258\n",
            "Epoch [10/100]\t Loss: 4.097352735201517\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.169574737548828\n",
            "Step [50/60]\t Loss: 3.9902024269104004\n",
            "Epoch [11/100]\t Loss: 4.068342121442159\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.008396148681641\n",
            "Step [50/60]\t Loss: 4.125629425048828\n",
            "Epoch [12/100]\t Loss: 4.065434817473093\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.128260612487793\n",
            "Step [50/60]\t Loss: 4.0638108253479\n",
            "Epoch [13/100]\t Loss: 4.047499493757884\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.245190620422363\n",
            "Step [50/60]\t Loss: 3.956733465194702\n",
            "Epoch [14/100]\t Loss: 4.046801761786143\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.21693229675293\n",
            "Step [50/60]\t Loss: 3.8905858993530273\n",
            "Epoch [15/100]\t Loss: 4.029264585177104\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.029198169708252\n",
            "Step [50/60]\t Loss: 3.9838457107543945\n",
            "Epoch [16/100]\t Loss: 4.021716499328614\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.26466178894043\n",
            "Step [50/60]\t Loss: 3.9739596843719482\n",
            "Epoch [17/100]\t Loss: 4.027491796016693\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.9307541847229004\n",
            "Step [50/60]\t Loss: 3.828334331512451\n",
            "Epoch [18/100]\t Loss: 3.983833368619283\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.0744242668151855\n",
            "Step [50/60]\t Loss: 3.95245623588562\n",
            "Epoch [19/100]\t Loss: 3.9859678705533343\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.9308021068573\n",
            "Step [50/60]\t Loss: 4.038267135620117\n",
            "Epoch [20/100]\t Loss: 3.9983126759529113\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.911572217941284\n",
            "Step [50/60]\t Loss: 3.8079729080200195\n",
            "Epoch [21/100]\t Loss: 3.9811198711395264\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.773313045501709\n",
            "Step [50/60]\t Loss: 3.951633930206299\n",
            "Epoch [22/100]\t Loss: 3.938235036532084\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.184584140777588\n",
            "Step [50/60]\t Loss: 3.8861541748046875\n",
            "Epoch [23/100]\t Loss: 3.9192339340845743\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.9922690391540527\n",
            "Step [50/60]\t Loss: 3.8981947898864746\n",
            "Epoch [24/100]\t Loss: 3.8854385813077292\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.7636075019836426\n",
            "Step [50/60]\t Loss: 4.018157005310059\n",
            "Epoch [25/100]\t Loss: 3.910888628164927\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.7509469985961914\n",
            "Step [50/60]\t Loss: 4.089630603790283\n",
            "Epoch [26/100]\t Loss: 3.8834562500317893\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.8542110919952393\n",
            "Step [50/60]\t Loss: 3.882699966430664\n",
            "Epoch [27/100]\t Loss: 3.828534309069316\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.860801935195923\n",
            "Step [50/60]\t Loss: 3.7905733585357666\n",
            "Epoch [28/100]\t Loss: 3.8658957759539287\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.601365089416504\n",
            "Step [50/60]\t Loss: 3.751260757446289\n",
            "Epoch [29/100]\t Loss: 3.8137455066045125\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.9084103107452393\n",
            "Step [50/60]\t Loss: 3.729234457015991\n",
            "Epoch [30/100]\t Loss: 3.8123650471369426\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.9570422172546387\n",
            "Step [50/60]\t Loss: 3.9190380573272705\n",
            "Epoch [31/100]\t Loss: 3.8230640172958372\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.739645004272461\n",
            "Step [50/60]\t Loss: 3.844754934310913\n",
            "Epoch [32/100]\t Loss: 3.7599035143852233\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.6173031330108643\n",
            "Step [50/60]\t Loss: 3.55784010887146\n",
            "Epoch [33/100]\t Loss: 3.701543629169464\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 4.1271514892578125\n",
            "Step [50/60]\t Loss: 3.695312738418579\n",
            "Epoch [34/100]\t Loss: 3.6981313705444334\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.788585662841797\n",
            "Step [50/60]\t Loss: 3.7103466987609863\n",
            "Epoch [35/100]\t Loss: 3.6995610356330872\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.624305486679077\n",
            "Step [50/60]\t Loss: 3.4837141036987305\n",
            "Epoch [36/100]\t Loss: 3.597859239578247\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.7693700790405273\n",
            "Step [50/60]\t Loss: 3.6511850357055664\n",
            "Epoch [37/100]\t Loss: 3.6971833308537803\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.553208112716675\n",
            "Step [50/60]\t Loss: 3.634948253631592\n",
            "Epoch [38/100]\t Loss: 3.605062433083852\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.3145980834960938\n",
            "Step [50/60]\t Loss: 3.4557385444641113\n",
            "Epoch [39/100]\t Loss: 3.4557708779970806\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.490473985671997\n",
            "Step [50/60]\t Loss: 3.398036003112793\n",
            "Epoch [40/100]\t Loss: 3.384928850332896\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.0730538368225098\n",
            "Step [50/60]\t Loss: 3.0944392681121826\n",
            "Epoch [41/100]\t Loss: 3.3270086884498595\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.1767971515655518\n",
            "Step [50/60]\t Loss: 3.255728006362915\n",
            "Epoch [42/100]\t Loss: 3.306581270694733\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.4062161445617676\n",
            "Step [50/60]\t Loss: 3.1149399280548096\n",
            "Epoch [43/100]\t Loss: 3.28633705774943\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.2227275371551514\n",
            "Step [50/60]\t Loss: 3.4674320220947266\n",
            "Epoch [44/100]\t Loss: 3.278606168429057\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.428079128265381\n",
            "Step [50/60]\t Loss: 3.216073513031006\n",
            "Epoch [45/100]\t Loss: 3.300786538918813\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.1600863933563232\n",
            "Step [50/60]\t Loss: 3.339263916015625\n",
            "Epoch [46/100]\t Loss: 3.244681998093923\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.260798215866089\n",
            "Step [50/60]\t Loss: 3.115967035293579\n",
            "Epoch [47/100]\t Loss: 3.1926297704378763\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.2180731296539307\n",
            "Step [50/60]\t Loss: 3.2505903244018555\n",
            "Epoch [48/100]\t Loss: 3.2092241485913595\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.1570913791656494\n",
            "Step [50/60]\t Loss: 3.1250219345092773\n",
            "Epoch [49/100]\t Loss: 3.2229805548985797\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.132988214492798\n",
            "Step [50/60]\t Loss: 3.2472987174987793\n",
            "Epoch [50/100]\t Loss: 3.235101270675659\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.2391908168792725\n",
            "Step [50/60]\t Loss: 3.1101393699645996\n",
            "Epoch [51/100]\t Loss: 3.180918749173482\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9640307426452637\n",
            "Step [50/60]\t Loss: 3.0479540824890137\n",
            "Epoch [52/100]\t Loss: 3.1768348097801207\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.1173510551452637\n",
            "Step [50/60]\t Loss: 3.1792593002319336\n",
            "Epoch [53/100]\t Loss: 3.157157337665558\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.945706605911255\n",
            "Step [50/60]\t Loss: 3.24242901802063\n",
            "Epoch [54/100]\t Loss: 3.151629900932312\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.433668613433838\n",
            "Step [50/60]\t Loss: 3.1795265674591064\n",
            "Epoch [55/100]\t Loss: 3.1490456660588584\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.179142951965332\n",
            "Step [50/60]\t Loss: 3.040259599685669\n",
            "Epoch [56/100]\t Loss: 3.1470377763112385\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.0994820594787598\n",
            "Step [50/60]\t Loss: 3.0875160694122314\n",
            "Epoch [57/100]\t Loss: 3.0929553945859274\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.1457204818725586\n",
            "Step [50/60]\t Loss: 3.208214044570923\n",
            "Epoch [58/100]\t Loss: 3.096236570676168\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.132913827896118\n",
            "Step [50/60]\t Loss: 2.9806458950042725\n",
            "Epoch [59/100]\t Loss: 3.0814780076344808\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.024235963821411\n",
            "Step [50/60]\t Loss: 3.0644071102142334\n",
            "Epoch [60/100]\t Loss: 3.069372630119324\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.0034186840057373\n",
            "Step [50/60]\t Loss: 3.084009885787964\n",
            "Epoch [61/100]\t Loss: 3.0456377704938253\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9798882007598877\n",
            "Step [50/60]\t Loss: 3.054046630859375\n",
            "Epoch [62/100]\t Loss: 3.0438446482022603\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.069364547729492\n",
            "Step [50/60]\t Loss: 2.953864336013794\n",
            "Epoch [63/100]\t Loss: 3.0417082985242208\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9549825191497803\n",
            "Step [50/60]\t Loss: 2.928640127182007\n",
            "Epoch [64/100]\t Loss: 3.0357511401176454\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9911575317382812\n",
            "Step [50/60]\t Loss: 2.9879493713378906\n",
            "Epoch [65/100]\t Loss: 3.0150123953819277\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9627885818481445\n",
            "Step [50/60]\t Loss: 3.063469648361206\n",
            "Epoch [66/100]\t Loss: 3.0309938549995423\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.199424982070923\n",
            "Step [50/60]\t Loss: 3.066913604736328\n",
            "Epoch [67/100]\t Loss: 3.0146111448605857\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9696438312530518\n",
            "Step [50/60]\t Loss: 3.10904598236084\n",
            "Epoch [68/100]\t Loss: 2.979465961456299\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.1856114864349365\n",
            "Step [50/60]\t Loss: 2.8843612670898438\n",
            "Epoch [69/100]\t Loss: 2.9893025557200112\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.0244715213775635\n",
            "Step [50/60]\t Loss: 2.973606824874878\n",
            "Epoch [70/100]\t Loss: 2.995220653216044\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.003082275390625\n",
            "Step [50/60]\t Loss: 2.9132213592529297\n",
            "Epoch [71/100]\t Loss: 3.00033282438914\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.993723154067993\n",
            "Step [50/60]\t Loss: 2.811021566390991\n",
            "Epoch [72/100]\t Loss: 2.97665665547053\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.81251859664917\n",
            "Step [50/60]\t Loss: 3.0174410343170166\n",
            "Epoch [73/100]\t Loss: 2.981029486656189\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.00430965423584\n",
            "Step [50/60]\t Loss: 3.024676561355591\n",
            "Epoch [74/100]\t Loss: 2.9791915973027545\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.973205089569092\n",
            "Step [50/60]\t Loss: 3.014510154724121\n",
            "Epoch [75/100]\t Loss: 2.9727717876434325\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.937310218811035\n",
            "Step [50/60]\t Loss: 3.0107498168945312\n",
            "Epoch [76/100]\t Loss: 2.9764909545580545\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.941664695739746\n",
            "Step [50/60]\t Loss: 3.0348165035247803\n",
            "Epoch [77/100]\t Loss: 2.951688814163208\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9292709827423096\n",
            "Step [50/60]\t Loss: 2.9077486991882324\n",
            "Epoch [78/100]\t Loss: 2.94386217991511\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.8474671840667725\n",
            "Step [50/60]\t Loss: 2.9011592864990234\n",
            "Epoch [79/100]\t Loss: 2.9501436948776245\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.1348061561584473\n",
            "Step [50/60]\t Loss: 3.134629964828491\n",
            "Epoch [80/100]\t Loss: 2.9726948976516723\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9383530616760254\n",
            "Step [50/60]\t Loss: 2.8233587741851807\n",
            "Epoch [81/100]\t Loss: 2.9556227723757424\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.8518495559692383\n",
            "Step [50/60]\t Loss: 3.05973482131958\n",
            "Epoch [82/100]\t Loss: 2.9421926895777384\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9237637519836426\n",
            "Step [50/60]\t Loss: 2.8827767372131348\n",
            "Epoch [83/100]\t Loss: 2.929109529654185\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9477946758270264\n",
            "Step [50/60]\t Loss: 2.951963186264038\n",
            "Epoch [84/100]\t Loss: 2.953083038330078\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.1138641834259033\n",
            "Step [50/60]\t Loss: 2.9792304039001465\n",
            "Epoch [85/100]\t Loss: 2.93341646194458\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9090631008148193\n",
            "Step [50/60]\t Loss: 3.053905963897705\n",
            "Epoch [86/100]\t Loss: 2.936719516913096\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.109828233718872\n",
            "Step [50/60]\t Loss: 2.9796016216278076\n",
            "Epoch [87/100]\t Loss: 2.9653629899024962\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.8473899364471436\n",
            "Step [50/60]\t Loss: 3.178010940551758\n",
            "Epoch [88/100]\t Loss: 2.9382051825523376\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.856020212173462\n",
            "Step [50/60]\t Loss: 3.0254759788513184\n",
            "Epoch [89/100]\t Loss: 2.915460526943207\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.813058376312256\n",
            "Step [50/60]\t Loss: 3.053239107131958\n",
            "Epoch [90/100]\t Loss: 2.94357062180837\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.792569160461426\n",
            "Step [50/60]\t Loss: 3.066227674484253\n",
            "Epoch [91/100]\t Loss: 2.94123029311498\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.9055569171905518\n",
            "Step [50/60]\t Loss: 2.8513972759246826\n",
            "Epoch [92/100]\t Loss: 2.915062646071116\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.8336668014526367\n",
            "Step [50/60]\t Loss: 2.866436004638672\n",
            "Epoch [93/100]\t Loss: 2.9203458865483602\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.086312770843506\n",
            "Step [50/60]\t Loss: 2.9922034740448\n",
            "Epoch [94/100]\t Loss: 2.9088394125302632\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.0372250080108643\n",
            "Step [50/60]\t Loss: 2.950268507003784\n",
            "Epoch [95/100]\t Loss: 2.915200686454773\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 3.025740623474121\n",
            "Step [50/60]\t Loss: 2.8739285469055176\n",
            "Epoch [96/100]\t Loss: 2.910641837120056\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.8154196739196777\n",
            "Step [50/60]\t Loss: 2.922025442123413\n",
            "Epoch [97/100]\t Loss: 2.9187710960706075\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.896000623703003\n",
            "Step [50/60]\t Loss: 2.900876522064209\n",
            "Epoch [98/100]\t Loss: 2.9035175879796347\t lr: 0.0003\n",
            "Step [0/60]\t Loss: 2.871328592300415\n",
            "Step [50/60]\t Loss: 2.8920488357543945\n",
            "Epoch [99/100]\t Loss: 2.9243792017300922\t lr: 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model/model.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "-aUBTbzPcAiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44486cc-6aba-4b43-afd8-28809bec5146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimCLR(\n",
              "  (encoder): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Identity()\n",
              "  )\n",
              "  (projector): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=2048, out_features=64, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import torch.nn as nn\n",
        " projector = nn.Sequential(\n",
        "            nn.Linear(model.n_features, model.n_features, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(model.n_features, 6, bias=False),\n",
        "            nn.Softmax()\n",
        "        )\n",
        " model.projector = projector"
      ],
      "metadata": {
        "id": "Xx18q0WQbFeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([#todo: more\n",
        "       # transforms.GaussianBlur(kernel_size=501),\n",
        "       # transforms.CenterCrop(10),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        #transforms.Resize((224, 224)),#attention\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                            [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "\n",
        "    'test':transforms.Compose([\n",
        "        #transforms.GaussianBlur(kernel_size=501),\n",
        "         #transforms.CenterCrop(10),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                            [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "LdTPdyXrdh-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets = {x: datasets.ImageFolder( dirs[x],   transform=data_transforms[x]) for x in ['train', 'test']}\n",
        "# load the data into batches\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True) for x in ['train',  'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x])\n",
        "                              for x in ['train', 'test']}"
      ],
      "metadata": {
        "id": "qphHH-_tdiEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, model):\n",
        "      super(Net, self).__init__()\n",
        "      self.model = model\n",
        "      #self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "      #self.dropout1 = nn.Dropout2d(0.25)\n",
        "      #self.dropout2 = nn.Dropout2d(0.5)\n",
        "      #self.fc1 = nn.Linear(9216, 128)\n",
        "      #self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "      # Pass data through conv1\n",
        "      h_i = self.model.encoder(x)\n",
        "       # h_j = self.encoder(x_j)\n",
        "\n",
        "      z_i = self.model.projector(h_i)\n",
        "        #z_j = self.projector(h_j)\n",
        "      return z_i\n",
        ""
      ],
      "metadata": {
        "id": "uD69D02tmWDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model1 = Net(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr = .0001)\n",
        "model1 = model1.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Y8d8Vrj_Zi",
        "outputId": "5a6dec04-46ed-40c4-8a69-421e121af0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def forward(self, x):\n",
        "#        h_i = model.encoder(x)\n",
        "#       # h_j = self.encoder(x_j)\n",
        "#\n",
        "#        z_i = model.projector(h_i)\n",
        "#        #z_j = self.projector(h_j)\n",
        "#        return z_i\n",
        "#model.forward = forward"
      ],
      "metadata": {
        "id": "-WxpAXXjkUQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    running_train_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "    running_vall_loss = 0.0\n",
        "    total = 0\n",
        "    ## model.eval() to model.train\n",
        "    model1.train()\n",
        "    torch.save(model1.state_dict(), os.path.join(\"/content/drive/MyDrive/save_train_simclr\", 'epoch-{}.pth'.format(epoch)))\n",
        "    for i, (images, labels) in enumerate(dataloaders['train']):\n",
        "        #print(\"PASSED!!\")\n",
        "        # Forward pass\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model1(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 5 == 0:\n",
        "          print (f'Epoch : {epoch+1} , Step : {i+1} , Loss: {loss.item():.4f}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5RM2OAMjENF",
        "outputId": "d1d4f4c2-cac4-42ac-b624-b6debfa15658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 , Step : 5 , Loss: 1.5552\n",
            "Epoch : 1 , Step : 10 , Loss: 1.5672\n",
            "Epoch : 1 , Step : 15 , Loss: 1.5641\n",
            "Epoch : 1 , Step : 20 , Loss: 1.4186\n",
            "Epoch : 1 , Step : 25 , Loss: 1.3916\n",
            "Epoch : 1 , Step : 30 , Loss: 1.3549\n",
            "Epoch : 1 , Step : 35 , Loss: 1.4165\n",
            "Epoch : 1 , Step : 40 , Loss: 1.5023\n",
            "Epoch : 1 , Step : 45 , Loss: 1.4158\n",
            "Epoch : 1 , Step : 50 , Loss: 1.4038\n",
            "Epoch : 1 , Step : 55 , Loss: 1.4825\n",
            "Epoch : 1 , Step : 60 , Loss: 1.4283\n",
            "Epoch : 2 , Step : 5 , Loss: 1.4280\n",
            "Epoch : 2 , Step : 10 , Loss: 1.4063\n",
            "Epoch : 2 , Step : 15 , Loss: 1.5151\n",
            "Epoch : 2 , Step : 20 , Loss: 1.4640\n",
            "Epoch : 2 , Step : 25 , Loss: 1.3228\n",
            "Epoch : 2 , Step : 30 , Loss: 1.3402\n",
            "Epoch : 2 , Step : 35 , Loss: 1.2545\n",
            "Epoch : 2 , Step : 40 , Loss: 1.4460\n",
            "Epoch : 2 , Step : 45 , Loss: 1.4897\n",
            "Epoch : 2 , Step : 50 , Loss: 1.4582\n",
            "Epoch : 2 , Step : 55 , Loss: 1.3761\n",
            "Epoch : 2 , Step : 60 , Loss: 1.2316\n",
            "Epoch : 3 , Step : 5 , Loss: 1.3608\n",
            "Epoch : 3 , Step : 10 , Loss: 1.5474\n",
            "Epoch : 3 , Step : 15 , Loss: 1.4733\n",
            "Epoch : 3 , Step : 20 , Loss: 1.5258\n",
            "Epoch : 3 , Step : 25 , Loss: 1.4751\n",
            "Epoch : 3 , Step : 30 , Loss: 1.4658\n",
            "Epoch : 3 , Step : 35 , Loss: 1.3525\n",
            "Epoch : 3 , Step : 40 , Loss: 1.4905\n",
            "Epoch : 3 , Step : 45 , Loss: 1.3403\n",
            "Epoch : 3 , Step : 50 , Loss: 1.3652\n",
            "Epoch : 3 , Step : 55 , Loss: 1.4379\n",
            "Epoch : 3 , Step : 60 , Loss: 1.3233\n",
            "Epoch : 4 , Step : 5 , Loss: 1.3290\n",
            "Epoch : 4 , Step : 10 , Loss: 1.4479\n",
            "Epoch : 4 , Step : 15 , Loss: 1.3615\n",
            "Epoch : 4 , Step : 20 , Loss: 1.3354\n",
            "Epoch : 4 , Step : 25 , Loss: 1.4173\n",
            "Epoch : 4 , Step : 30 , Loss: 1.3789\n",
            "Epoch : 4 , Step : 35 , Loss: 1.3424\n",
            "Epoch : 4 , Step : 40 , Loss: 1.5941\n",
            "Epoch : 4 , Step : 45 , Loss: 1.4901\n",
            "Epoch : 4 , Step : 50 , Loss: 1.4534\n",
            "Epoch : 4 , Step : 55 , Loss: 1.2909\n",
            "Epoch : 4 , Step : 60 , Loss: 1.3629\n",
            "Epoch : 5 , Step : 5 , Loss: 1.4527\n",
            "Epoch : 5 , Step : 10 , Loss: 1.3599\n",
            "Epoch : 5 , Step : 15 , Loss: 1.5261\n",
            "Epoch : 5 , Step : 20 , Loss: 1.2919\n",
            "Epoch : 5 , Step : 25 , Loss: 1.3171\n",
            "Epoch : 5 , Step : 30 , Loss: 1.3821\n",
            "Epoch : 5 , Step : 35 , Loss: 1.3639\n",
            "Epoch : 5 , Step : 40 , Loss: 1.2935\n",
            "Epoch : 5 , Step : 45 , Loss: 1.4629\n",
            "Epoch : 5 , Step : 50 , Loss: 1.4744\n",
            "Epoch : 5 , Step : 55 , Loss: 1.4008\n",
            "Epoch : 5 , Step : 60 , Loss: 1.3440\n",
            "Epoch : 6 , Step : 5 , Loss: 1.3370\n",
            "Epoch : 6 , Step : 10 , Loss: 1.4915\n",
            "Epoch : 6 , Step : 15 , Loss: 1.3475\n",
            "Epoch : 6 , Step : 20 , Loss: 1.2709\n",
            "Epoch : 6 , Step : 25 , Loss: 1.2449\n",
            "Epoch : 6 , Step : 30 , Loss: 1.3890\n",
            "Epoch : 6 , Step : 35 , Loss: 1.3013\n",
            "Epoch : 6 , Step : 40 , Loss: 1.3830\n",
            "Epoch : 6 , Step : 45 , Loss: 1.3509\n",
            "Epoch : 6 , Step : 50 , Loss: 1.3949\n",
            "Epoch : 6 , Step : 55 , Loss: 1.3753\n",
            "Epoch : 6 , Step : 60 , Loss: 1.4644\n",
            "Epoch : 7 , Step : 5 , Loss: 1.3526\n",
            "Epoch : 7 , Step : 10 , Loss: 1.4262\n",
            "Epoch : 7 , Step : 15 , Loss: 1.4794\n",
            "Epoch : 7 , Step : 20 , Loss: 1.3121\n",
            "Epoch : 7 , Step : 25 , Loss: 1.4056\n",
            "Epoch : 7 , Step : 30 , Loss: 1.2843\n",
            "Epoch : 7 , Step : 35 , Loss: 1.3198\n",
            "Epoch : 7 , Step : 40 , Loss: 1.2895\n",
            "Epoch : 7 , Step : 45 , Loss: 1.4163\n",
            "Epoch : 7 , Step : 50 , Loss: 1.4114\n",
            "Epoch : 7 , Step : 55 , Loss: 1.3273\n",
            "Epoch : 7 , Step : 60 , Loss: 1.4479\n",
            "Epoch : 8 , Step : 5 , Loss: 1.4097\n",
            "Epoch : 8 , Step : 10 , Loss: 1.2766\n",
            "Epoch : 8 , Step : 15 , Loss: 1.4069\n",
            "Epoch : 8 , Step : 20 , Loss: 1.2846\n",
            "Epoch : 8 , Step : 25 , Loss: 1.2286\n",
            "Epoch : 8 , Step : 30 , Loss: 1.2154\n",
            "Epoch : 8 , Step : 35 , Loss: 1.2305\n",
            "Epoch : 8 , Step : 40 , Loss: 1.3676\n",
            "Epoch : 8 , Step : 45 , Loss: 1.2910\n",
            "Epoch : 8 , Step : 50 , Loss: 1.3148\n",
            "Epoch : 8 , Step : 55 , Loss: 1.3242\n",
            "Epoch : 8 , Step : 60 , Loss: 1.4035\n",
            "Epoch : 9 , Step : 5 , Loss: 1.3680\n",
            "Epoch : 9 , Step : 10 , Loss: 1.5720\n",
            "Epoch : 9 , Step : 15 , Loss: 1.2688\n",
            "Epoch : 9 , Step : 20 , Loss: 1.3512\n",
            "Epoch : 9 , Step : 25 , Loss: 1.4260\n",
            "Epoch : 9 , Step : 30 , Loss: 1.3710\n",
            "Epoch : 9 , Step : 35 , Loss: 1.4237\n",
            "Epoch : 9 , Step : 40 , Loss: 1.3182\n",
            "Epoch : 9 , Step : 45 , Loss: 1.4243\n",
            "Epoch : 9 , Step : 50 , Loss: 1.3488\n",
            "Epoch : 9 , Step : 55 , Loss: 1.5101\n",
            "Epoch : 9 , Step : 60 , Loss: 1.3562\n",
            "Epoch : 10 , Step : 5 , Loss: 1.2958\n",
            "Epoch : 10 , Step : 10 , Loss: 1.4107\n",
            "Epoch : 10 , Step : 15 , Loss: 1.4440\n",
            "Epoch : 10 , Step : 20 , Loss: 1.2572\n",
            "Epoch : 10 , Step : 25 , Loss: 1.3781\n",
            "Epoch : 10 , Step : 30 , Loss: 1.3269\n",
            "Epoch : 10 , Step : 35 , Loss: 1.4047\n",
            "Epoch : 10 , Step : 40 , Loss: 1.3727\n",
            "Epoch : 10 , Step : 45 , Loss: 1.2735\n",
            "Epoch : 10 , Step : 50 , Loss: 1.2837\n",
            "Epoch : 10 , Step : 55 , Loss: 1.3618\n",
            "Epoch : 10 , Step : 60 , Loss: 1.3827\n",
            "Epoch : 11 , Step : 5 , Loss: 1.2721\n",
            "Epoch : 11 , Step : 10 , Loss: 1.2280\n",
            "Epoch : 11 , Step : 15 , Loss: 1.2723\n",
            "Epoch : 11 , Step : 20 , Loss: 1.2852\n",
            "Epoch : 11 , Step : 25 , Loss: 1.3168\n",
            "Epoch : 11 , Step : 30 , Loss: 1.4584\n",
            "Epoch : 11 , Step : 35 , Loss: 1.4181\n",
            "Epoch : 11 , Step : 40 , Loss: 1.2948\n",
            "Epoch : 11 , Step : 45 , Loss: 1.3097\n",
            "Epoch : 11 , Step : 50 , Loss: 1.2844\n",
            "Epoch : 11 , Step : 55 , Loss: 1.2972\n",
            "Epoch : 11 , Step : 60 , Loss: 1.3168\n",
            "Epoch : 12 , Step : 5 , Loss: 1.2912\n",
            "Epoch : 12 , Step : 10 , Loss: 1.2630\n",
            "Epoch : 12 , Step : 15 , Loss: 1.2250\n",
            "Epoch : 12 , Step : 20 , Loss: 1.4061\n",
            "Epoch : 12 , Step : 25 , Loss: 1.4144\n",
            "Epoch : 12 , Step : 30 , Loss: 1.2922\n",
            "Epoch : 12 , Step : 35 , Loss: 1.2896\n",
            "Epoch : 12 , Step : 40 , Loss: 1.3535\n",
            "Epoch : 12 , Step : 45 , Loss: 1.2610\n",
            "Epoch : 12 , Step : 50 , Loss: 1.2853\n",
            "Epoch : 12 , Step : 55 , Loss: 1.3710\n",
            "Epoch : 12 , Step : 60 , Loss: 1.3932\n",
            "Epoch : 13 , Step : 5 , Loss: 1.3567\n",
            "Epoch : 13 , Step : 10 , Loss: 1.3247\n",
            "Epoch : 13 , Step : 15 , Loss: 1.1996\n",
            "Epoch : 13 , Step : 20 , Loss: 1.2634\n",
            "Epoch : 13 , Step : 25 , Loss: 1.4380\n",
            "Epoch : 13 , Step : 30 , Loss: 1.2993\n",
            "Epoch : 13 , Step : 35 , Loss: 1.3926\n",
            "Epoch : 13 , Step : 40 , Loss: 1.2697\n",
            "Epoch : 13 , Step : 45 , Loss: 1.2651\n",
            "Epoch : 13 , Step : 50 , Loss: 1.3427\n",
            "Epoch : 13 , Step : 55 , Loss: 1.4334\n",
            "Epoch : 13 , Step : 60 , Loss: 1.2764\n",
            "Epoch : 14 , Step : 5 , Loss: 1.3938\n",
            "Epoch : 14 , Step : 10 , Loss: 1.3122\n",
            "Epoch : 14 , Step : 15 , Loss: 1.3538\n",
            "Epoch : 14 , Step : 20 , Loss: 1.2412\n",
            "Epoch : 14 , Step : 25 , Loss: 1.2877\n",
            "Epoch : 14 , Step : 30 , Loss: 1.1997\n",
            "Epoch : 14 , Step : 35 , Loss: 1.2599\n",
            "Epoch : 14 , Step : 40 , Loss: 1.4407\n",
            "Epoch : 14 , Step : 45 , Loss: 1.3727\n",
            "Epoch : 14 , Step : 50 , Loss: 1.3340\n",
            "Epoch : 14 , Step : 55 , Loss: 1.4540\n",
            "Epoch : 14 , Step : 60 , Loss: 1.3956\n",
            "Epoch : 15 , Step : 5 , Loss: 1.2586\n",
            "Epoch : 15 , Step : 10 , Loss: 1.2628\n",
            "Epoch : 15 , Step : 15 , Loss: 1.3608\n",
            "Epoch : 15 , Step : 20 , Loss: 1.4146\n",
            "Epoch : 15 , Step : 25 , Loss: 1.2304\n",
            "Epoch : 15 , Step : 30 , Loss: 1.2242\n",
            "Epoch : 15 , Step : 35 , Loss: 1.4260\n",
            "Epoch : 15 , Step : 40 , Loss: 1.4147\n",
            "Epoch : 15 , Step : 45 , Loss: 1.3071\n",
            "Epoch : 15 , Step : 50 , Loss: 1.2935\n",
            "Epoch : 15 , Step : 55 , Loss: 1.2050\n",
            "Epoch : 15 , Step : 60 , Loss: 1.4875\n",
            "Epoch : 16 , Step : 5 , Loss: 1.3238\n",
            "Epoch : 16 , Step : 10 , Loss: 1.3871\n",
            "Epoch : 16 , Step : 15 , Loss: 1.2279\n",
            "Epoch : 16 , Step : 20 , Loss: 1.2518\n",
            "Epoch : 16 , Step : 25 , Loss: 1.2505\n",
            "Epoch : 16 , Step : 30 , Loss: 1.3537\n",
            "Epoch : 16 , Step : 35 , Loss: 1.2530\n",
            "Epoch : 16 , Step : 40 , Loss: 1.3617\n",
            "Epoch : 16 , Step : 45 , Loss: 1.3181\n",
            "Epoch : 16 , Step : 50 , Loss: 1.2449\n",
            "Epoch : 16 , Step : 55 , Loss: 1.3845\n",
            "Epoch : 16 , Step : 60 , Loss: 1.2627\n",
            "Epoch : 17 , Step : 5 , Loss: 1.3753\n",
            "Epoch : 17 , Step : 10 , Loss: 1.3787\n",
            "Epoch : 17 , Step : 15 , Loss: 1.3042\n",
            "Epoch : 17 , Step : 20 , Loss: 1.3793\n",
            "Epoch : 17 , Step : 25 , Loss: 1.3092\n",
            "Epoch : 17 , Step : 30 , Loss: 1.2759\n",
            "Epoch : 17 , Step : 35 , Loss: 1.2876\n",
            "Epoch : 17 , Step : 40 , Loss: 1.2933\n",
            "Epoch : 17 , Step : 45 , Loss: 1.3676\n",
            "Epoch : 17 , Step : 50 , Loss: 1.2861\n",
            "Epoch : 17 , Step : 55 , Loss: 1.3482\n",
            "Epoch : 17 , Step : 60 , Loss: 1.3712\n",
            "Epoch : 18 , Step : 5 , Loss: 1.2852\n",
            "Epoch : 18 , Step : 10 , Loss: 1.2734\n",
            "Epoch : 18 , Step : 15 , Loss: 1.1911\n",
            "Epoch : 18 , Step : 20 , Loss: 1.3966\n",
            "Epoch : 18 , Step : 25 , Loss: 1.2768\n",
            "Epoch : 18 , Step : 30 , Loss: 1.2358\n",
            "Epoch : 18 , Step : 35 , Loss: 1.1995\n",
            "Epoch : 18 , Step : 40 , Loss: 1.3239\n",
            "Epoch : 18 , Step : 45 , Loss: 1.3907\n",
            "Epoch : 18 , Step : 50 , Loss: 1.2072\n",
            "Epoch : 18 , Step : 55 , Loss: 1.2621\n",
            "Epoch : 18 , Step : 60 , Loss: 1.2498\n",
            "Epoch : 19 , Step : 5 , Loss: 1.3811\n",
            "Epoch : 19 , Step : 10 , Loss: 1.3274\n",
            "Epoch : 19 , Step : 15 , Loss: 1.3360\n",
            "Epoch : 19 , Step : 20 , Loss: 1.2820\n",
            "Epoch : 19 , Step : 25 , Loss: 1.2526\n",
            "Epoch : 19 , Step : 30 , Loss: 1.1743\n",
            "Epoch : 19 , Step : 35 , Loss: 1.2396\n",
            "Epoch : 19 , Step : 40 , Loss: 1.3895\n",
            "Epoch : 19 , Step : 45 , Loss: 1.3526\n",
            "Epoch : 19 , Step : 50 , Loss: 1.2684\n",
            "Epoch : 19 , Step : 55 , Loss: 1.2585\n",
            "Epoch : 19 , Step : 60 , Loss: 1.4079\n",
            "Epoch : 20 , Step : 5 , Loss: 1.2544\n",
            "Epoch : 20 , Step : 10 , Loss: 1.2680\n",
            "Epoch : 20 , Step : 15 , Loss: 1.4254\n",
            "Epoch : 20 , Step : 20 , Loss: 1.3057\n",
            "Epoch : 20 , Step : 25 , Loss: 1.1857\n",
            "Epoch : 20 , Step : 30 , Loss: 1.2640\n",
            "Epoch : 20 , Step : 35 , Loss: 1.4529\n",
            "Epoch : 20 , Step : 40 , Loss: 1.3202\n",
            "Epoch : 20 , Step : 45 , Loss: 1.3340\n",
            "Epoch : 20 , Step : 50 , Loss: 1.2477\n",
            "Epoch : 20 , Step : 55 , Loss: 1.2685\n",
            "Epoch : 20 , Step : 60 , Loss: 1.2497\n",
            "Epoch : 21 , Step : 5 , Loss: 1.3107\n",
            "Epoch : 21 , Step : 10 , Loss: 1.3877\n",
            "Epoch : 21 , Step : 15 , Loss: 1.3430\n",
            "Epoch : 21 , Step : 20 , Loss: 1.2412\n",
            "Epoch : 21 , Step : 25 , Loss: 1.4559\n",
            "Epoch : 21 , Step : 30 , Loss: 1.4106\n",
            "Epoch : 21 , Step : 35 , Loss: 1.4179\n",
            "Epoch : 21 , Step : 40 , Loss: 1.3477\n",
            "Epoch : 21 , Step : 45 , Loss: 1.2909\n",
            "Epoch : 21 , Step : 50 , Loss: 1.3090\n",
            "Epoch : 21 , Step : 55 , Loss: 1.3842\n",
            "Epoch : 21 , Step : 60 , Loss: 1.2661\n",
            "Epoch : 22 , Step : 5 , Loss: 1.3388\n",
            "Epoch : 22 , Step : 10 , Loss: 1.4260\n",
            "Epoch : 22 , Step : 15 , Loss: 1.3502\n",
            "Epoch : 22 , Step : 20 , Loss: 1.3760\n",
            "Epoch : 22 , Step : 25 , Loss: 1.3228\n",
            "Epoch : 22 , Step : 30 , Loss: 1.2298\n",
            "Epoch : 22 , Step : 35 , Loss: 1.3075\n",
            "Epoch : 22 , Step : 40 , Loss: 1.3377\n",
            "Epoch : 22 , Step : 45 , Loss: 1.2744\n",
            "Epoch : 22 , Step : 50 , Loss: 1.3040\n",
            "Epoch : 22 , Step : 55 , Loss: 1.3223\n",
            "Epoch : 22 , Step : 60 , Loss: 1.2540\n",
            "Epoch : 23 , Step : 5 , Loss: 1.1930\n",
            "Epoch : 23 , Step : 10 , Loss: 1.4050\n",
            "Epoch : 23 , Step : 15 , Loss: 1.3672\n",
            "Epoch : 23 , Step : 20 , Loss: 1.2606\n",
            "Epoch : 23 , Step : 25 , Loss: 1.3377\n",
            "Epoch : 23 , Step : 30 , Loss: 1.2904\n",
            "Epoch : 23 , Step : 35 , Loss: 1.1510\n",
            "Epoch : 23 , Step : 40 , Loss: 1.2265\n",
            "Epoch : 23 , Step : 45 , Loss: 1.2006\n",
            "Epoch : 23 , Step : 50 , Loss: 1.1959\n",
            "Epoch : 23 , Step : 55 , Loss: 1.3338\n",
            "Epoch : 23 , Step : 60 , Loss: 1.2308\n",
            "Epoch : 24 , Step : 5 , Loss: 1.2054\n",
            "Epoch : 24 , Step : 10 , Loss: 1.2080\n",
            "Epoch : 24 , Step : 15 , Loss: 1.1840\n",
            "Epoch : 24 , Step : 20 , Loss: 1.4553\n",
            "Epoch : 24 , Step : 25 , Loss: 1.2620\n",
            "Epoch : 24 , Step : 30 , Loss: 1.2560\n",
            "Epoch : 24 , Step : 35 , Loss: 1.2659\n",
            "Epoch : 24 , Step : 40 , Loss: 1.2388\n",
            "Epoch : 24 , Step : 45 , Loss: 1.3332\n",
            "Epoch : 24 , Step : 50 , Loss: 1.5014\n",
            "Epoch : 24 , Step : 55 , Loss: 1.2311\n",
            "Epoch : 24 , Step : 60 , Loss: 1.3130\n",
            "Epoch : 25 , Step : 5 , Loss: 1.2219\n",
            "Epoch : 25 , Step : 10 , Loss: 1.3031\n",
            "Epoch : 25 , Step : 15 , Loss: 1.2291\n",
            "Epoch : 25 , Step : 20 , Loss: 1.2897\n",
            "Epoch : 25 , Step : 25 , Loss: 1.3217\n",
            "Epoch : 25 , Step : 30 , Loss: 1.3038\n",
            "Epoch : 25 , Step : 35 , Loss: 1.2040\n",
            "Epoch : 25 , Step : 40 , Loss: 1.4103\n",
            "Epoch : 25 , Step : 45 , Loss: 1.3566\n",
            "Epoch : 25 , Step : 50 , Loss: 1.1115\n",
            "Epoch : 25 , Step : 55 , Loss: 1.3575\n",
            "Epoch : 25 , Step : 60 , Loss: 1.4005\n",
            "Epoch : 26 , Step : 5 , Loss: 1.2233\n",
            "Epoch : 26 , Step : 10 , Loss: 1.2013\n",
            "Epoch : 26 , Step : 15 , Loss: 1.3367\n",
            "Epoch : 26 , Step : 20 , Loss: 1.2849\n",
            "Epoch : 26 , Step : 25 , Loss: 1.2366\n",
            "Epoch : 26 , Step : 30 , Loss: 1.2792\n",
            "Epoch : 26 , Step : 35 , Loss: 1.2324\n",
            "Epoch : 26 , Step : 40 , Loss: 1.2172\n",
            "Epoch : 26 , Step : 45 , Loss: 1.3746\n",
            "Epoch : 26 , Step : 50 , Loss: 1.4052\n",
            "Epoch : 26 , Step : 55 , Loss: 1.2913\n",
            "Epoch : 26 , Step : 60 , Loss: 1.3811\n",
            "Epoch : 27 , Step : 5 , Loss: 1.1989\n",
            "Epoch : 27 , Step : 10 , Loss: 1.3574\n",
            "Epoch : 27 , Step : 15 , Loss: 1.3302\n",
            "Epoch : 27 , Step : 20 , Loss: 1.3564\n",
            "Epoch : 27 , Step : 25 , Loss: 1.2736\n",
            "Epoch : 27 , Step : 30 , Loss: 1.1695\n",
            "Epoch : 27 , Step : 35 , Loss: 1.1758\n",
            "Epoch : 27 , Step : 40 , Loss: 1.2959\n",
            "Epoch : 27 , Step : 45 , Loss: 1.3390\n",
            "Epoch : 27 , Step : 50 , Loss: 1.2322\n",
            "Epoch : 27 , Step : 55 , Loss: 1.3716\n",
            "Epoch : 27 , Step : 60 , Loss: 1.3454\n",
            "Epoch : 28 , Step : 5 , Loss: 1.2906\n",
            "Epoch : 28 , Step : 10 , Loss: 1.3300\n",
            "Epoch : 28 , Step : 15 , Loss: 1.1726\n",
            "Epoch : 28 , Step : 20 , Loss: 1.1949\n",
            "Epoch : 28 , Step : 25 , Loss: 1.2847\n",
            "Epoch : 28 , Step : 30 , Loss: 1.2939\n",
            "Epoch : 28 , Step : 35 , Loss: 1.2311\n",
            "Epoch : 28 , Step : 40 , Loss: 1.2744\n",
            "Epoch : 28 , Step : 45 , Loss: 1.3180\n",
            "Epoch : 28 , Step : 50 , Loss: 1.4485\n",
            "Epoch : 28 , Step : 55 , Loss: 1.1622\n",
            "Epoch : 28 , Step : 60 , Loss: 1.3011\n",
            "Epoch : 29 , Step : 5 , Loss: 1.2622\n",
            "Epoch : 29 , Step : 10 , Loss: 1.3119\n",
            "Epoch : 29 , Step : 15 , Loss: 1.4706\n",
            "Epoch : 29 , Step : 20 , Loss: 1.3292\n",
            "Epoch : 29 , Step : 25 , Loss: 1.2645\n",
            "Epoch : 29 , Step : 30 , Loss: 1.2527\n",
            "Epoch : 29 , Step : 35 , Loss: 1.3300\n",
            "Epoch : 29 , Step : 40 , Loss: 1.2412\n",
            "Epoch : 29 , Step : 45 , Loss: 1.2335\n",
            "Epoch : 29 , Step : 50 , Loss: 1.2392\n",
            "Epoch : 29 , Step : 55 , Loss: 1.3857\n",
            "Epoch : 29 , Step : 60 , Loss: 1.3758\n",
            "Epoch : 30 , Step : 5 , Loss: 1.4199\n",
            "Epoch : 30 , Step : 10 , Loss: 1.1999\n",
            "Epoch : 30 , Step : 15 , Loss: 1.2749\n",
            "Epoch : 30 , Step : 20 , Loss: 1.3914\n",
            "Epoch : 30 , Step : 25 , Loss: 1.3636\n",
            "Epoch : 30 , Step : 30 , Loss: 1.3200\n",
            "Epoch : 30 , Step : 35 , Loss: 1.2432\n",
            "Epoch : 30 , Step : 40 , Loss: 1.2357\n",
            "Epoch : 30 , Step : 45 , Loss: 1.2826\n",
            "Epoch : 30 , Step : 50 , Loss: 1.1742\n",
            "Epoch : 30 , Step : 55 , Loss: 1.3351\n",
            "Epoch : 30 , Step : 60 , Loss: 1.1061\n",
            "Epoch : 31 , Step : 5 , Loss: 1.3168\n",
            "Epoch : 31 , Step : 10 , Loss: 1.1961\n",
            "Epoch : 31 , Step : 15 , Loss: 1.2614\n",
            "Epoch : 31 , Step : 20 , Loss: 1.2387\n",
            "Epoch : 31 , Step : 25 , Loss: 1.2154\n",
            "Epoch : 31 , Step : 30 , Loss: 1.3016\n",
            "Epoch : 31 , Step : 35 , Loss: 1.2815\n",
            "Epoch : 31 , Step : 40 , Loss: 1.3925\n",
            "Epoch : 31 , Step : 45 , Loss: 1.4455\n",
            "Epoch : 31 , Step : 50 , Loss: 1.2834\n",
            "Epoch : 31 , Step : 55 , Loss: 1.2242\n",
            "Epoch : 31 , Step : 60 , Loss: 1.3489\n",
            "Epoch : 32 , Step : 5 , Loss: 1.3152\n",
            "Epoch : 32 , Step : 10 , Loss: 1.2619\n",
            "Epoch : 32 , Step : 15 , Loss: 1.2383\n",
            "Epoch : 32 , Step : 20 , Loss: 1.2298\n",
            "Epoch : 32 , Step : 25 , Loss: 1.2009\n",
            "Epoch : 32 , Step : 30 , Loss: 1.1990\n",
            "Epoch : 32 , Step : 35 , Loss: 1.1978\n",
            "Epoch : 32 , Step : 40 , Loss: 1.2755\n",
            "Epoch : 32 , Step : 45 , Loss: 1.2466\n",
            "Epoch : 32 , Step : 50 , Loss: 1.4136\n",
            "Epoch : 32 , Step : 55 , Loss: 1.4434\n",
            "Epoch : 32 , Step : 60 , Loss: 1.2151\n",
            "Epoch : 33 , Step : 5 , Loss: 1.1382\n",
            "Epoch : 33 , Step : 10 , Loss: 1.2955\n",
            "Epoch : 33 , Step : 15 , Loss: 1.3521\n",
            "Epoch : 33 , Step : 20 , Loss: 1.4047\n",
            "Epoch : 33 , Step : 25 , Loss: 1.2744\n",
            "Epoch : 33 , Step : 30 , Loss: 1.3759\n",
            "Epoch : 33 , Step : 35 , Loss: 1.2380\n",
            "Epoch : 33 , Step : 40 , Loss: 1.4500\n",
            "Epoch : 33 , Step : 45 , Loss: 1.4156\n",
            "Epoch : 33 , Step : 50 , Loss: 1.3252\n",
            "Epoch : 33 , Step : 55 , Loss: 1.3278\n",
            "Epoch : 33 , Step : 60 , Loss: 1.3295\n",
            "Epoch : 34 , Step : 5 , Loss: 1.2042\n",
            "Epoch : 34 , Step : 10 , Loss: 1.2411\n",
            "Epoch : 34 , Step : 15 , Loss: 1.2873\n",
            "Epoch : 34 , Step : 20 , Loss: 1.2005\n",
            "Epoch : 34 , Step : 25 , Loss: 1.2440\n",
            "Epoch : 34 , Step : 30 , Loss: 1.3216\n",
            "Epoch : 34 , Step : 35 , Loss: 1.3365\n",
            "Epoch : 34 , Step : 40 , Loss: 1.2559\n",
            "Epoch : 34 , Step : 45 , Loss: 1.3525\n",
            "Epoch : 34 , Step : 50 , Loss: 1.4305\n",
            "Epoch : 34 , Step : 55 , Loss: 1.1990\n",
            "Epoch : 34 , Step : 60 , Loss: 1.2311\n",
            "Epoch : 35 , Step : 5 , Loss: 1.3681\n",
            "Epoch : 35 , Step : 10 , Loss: 1.2447\n",
            "Epoch : 35 , Step : 15 , Loss: 1.2377\n",
            "Epoch : 35 , Step : 20 , Loss: 1.2945\n",
            "Epoch : 35 , Step : 25 , Loss: 1.2001\n",
            "Epoch : 35 , Step : 30 , Loss: 1.2018\n",
            "Epoch : 35 , Step : 35 , Loss: 1.3668\n",
            "Epoch : 35 , Step : 40 , Loss: 1.3141\n",
            "Epoch : 35 , Step : 45 , Loss: 1.2311\n",
            "Epoch : 35 , Step : 50 , Loss: 1.1865\n",
            "Epoch : 35 , Step : 55 , Loss: 1.2697\n",
            "Epoch : 35 , Step : 60 , Loss: 1.2197\n",
            "Epoch : 36 , Step : 5 , Loss: 1.2032\n",
            "Epoch : 36 , Step : 10 , Loss: 1.2007\n",
            "Epoch : 36 , Step : 15 , Loss: 1.2927\n",
            "Epoch : 36 , Step : 20 , Loss: 1.2587\n",
            "Epoch : 36 , Step : 25 , Loss: 1.3048\n",
            "Epoch : 36 , Step : 30 , Loss: 1.2449\n",
            "Epoch : 36 , Step : 35 , Loss: 1.4054\n",
            "Epoch : 36 , Step : 40 , Loss: 1.2118\n",
            "Epoch : 36 , Step : 45 , Loss: 1.1951\n",
            "Epoch : 36 , Step : 50 , Loss: 1.3102\n",
            "Epoch : 36 , Step : 55 , Loss: 1.2021\n",
            "Epoch : 36 , Step : 60 , Loss: 1.3099\n",
            "Epoch : 37 , Step : 5 , Loss: 1.2406\n",
            "Epoch : 37 , Step : 10 , Loss: 1.2009\n",
            "Epoch : 37 , Step : 15 , Loss: 1.2755\n",
            "Epoch : 37 , Step : 20 , Loss: 1.2674\n",
            "Epoch : 37 , Step : 25 , Loss: 1.1697\n",
            "Epoch : 37 , Step : 30 , Loss: 1.3898\n",
            "Epoch : 37 , Step : 35 , Loss: 1.2340\n",
            "Epoch : 37 , Step : 40 , Loss: 1.2588\n",
            "Epoch : 37 , Step : 45 , Loss: 1.2022\n",
            "Epoch : 37 , Step : 50 , Loss: 1.3502\n",
            "Epoch : 37 , Step : 55 , Loss: 1.2910\n",
            "Epoch : 37 , Step : 60 , Loss: 1.1975\n",
            "Epoch : 38 , Step : 5 , Loss: 1.2626\n",
            "Epoch : 38 , Step : 10 , Loss: 1.1686\n",
            "Epoch : 38 , Step : 15 , Loss: 1.2566\n",
            "Epoch : 38 , Step : 20 , Loss: 1.1655\n",
            "Epoch : 38 , Step : 25 , Loss: 1.2124\n",
            "Epoch : 38 , Step : 30 , Loss: 1.4240\n",
            "Epoch : 38 , Step : 35 , Loss: 1.2412\n",
            "Epoch : 38 , Step : 40 , Loss: 1.2932\n",
            "Epoch : 38 , Step : 45 , Loss: 1.2203\n",
            "Epoch : 38 , Step : 50 , Loss: 1.3053\n",
            "Epoch : 38 , Step : 55 , Loss: 1.2283\n",
            "Epoch : 38 , Step : 60 , Loss: 1.3036\n",
            "Epoch : 39 , Step : 5 , Loss: 1.2332\n",
            "Epoch : 39 , Step : 10 , Loss: 1.2514\n",
            "Epoch : 39 , Step : 15 , Loss: 1.3624\n",
            "Epoch : 39 , Step : 20 , Loss: 1.2455\n",
            "Epoch : 39 , Step : 25 , Loss: 1.2407\n",
            "Epoch : 39 , Step : 30 , Loss: 1.2710\n",
            "Epoch : 39 , Step : 35 , Loss: 1.2187\n",
            "Epoch : 39 , Step : 40 , Loss: 1.1798\n",
            "Epoch : 39 , Step : 45 , Loss: 1.2800\n",
            "Epoch : 39 , Step : 50 , Loss: 1.3362\n",
            "Epoch : 39 , Step : 55 , Loss: 1.2254\n",
            "Epoch : 39 , Step : 60 , Loss: 1.2269\n",
            "Epoch : 40 , Step : 5 , Loss: 1.2934\n",
            "Epoch : 40 , Step : 10 , Loss: 1.2262\n",
            "Epoch : 40 , Step : 15 , Loss: 1.2919\n",
            "Epoch : 40 , Step : 20 , Loss: 1.2900\n",
            "Epoch : 40 , Step : 25 , Loss: 1.1772\n",
            "Epoch : 40 , Step : 30 , Loss: 1.3065\n",
            "Epoch : 40 , Step : 35 , Loss: 1.3167\n",
            "Epoch : 40 , Step : 40 , Loss: 1.4604\n",
            "Epoch : 40 , Step : 45 , Loss: 1.2316\n",
            "Epoch : 40 , Step : 50 , Loss: 1.3181\n",
            "Epoch : 40 , Step : 55 , Loss: 1.2474\n",
            "Epoch : 40 , Step : 60 , Loss: 1.2522\n",
            "Epoch : 41 , Step : 5 , Loss: 1.3871\n",
            "Epoch : 41 , Step : 10 , Loss: 1.2298\n",
            "Epoch : 41 , Step : 15 , Loss: 1.1996\n",
            "Epoch : 41 , Step : 20 , Loss: 1.2139\n",
            "Epoch : 41 , Step : 25 , Loss: 1.2633\n",
            "Epoch : 41 , Step : 30 , Loss: 1.3840\n",
            "Epoch : 41 , Step : 35 , Loss: 1.2990\n",
            "Epoch : 41 , Step : 40 , Loss: 1.3068\n",
            "Epoch : 41 , Step : 45 , Loss: 1.3058\n",
            "Epoch : 41 , Step : 50 , Loss: 1.3715\n",
            "Epoch : 41 , Step : 55 , Loss: 1.3566\n",
            "Epoch : 41 , Step : 60 , Loss: 1.3051\n",
            "Epoch : 42 , Step : 5 , Loss: 1.2567\n",
            "Epoch : 42 , Step : 10 , Loss: 1.2602\n",
            "Epoch : 42 , Step : 15 , Loss: 1.2616\n",
            "Epoch : 42 , Step : 20 , Loss: 1.3050\n",
            "Epoch : 42 , Step : 25 , Loss: 1.2240\n",
            "Epoch : 42 , Step : 30 , Loss: 1.1858\n",
            "Epoch : 42 , Step : 35 , Loss: 1.2437\n",
            "Epoch : 42 , Step : 40 , Loss: 1.2121\n",
            "Epoch : 42 , Step : 45 , Loss: 1.3128\n",
            "Epoch : 42 , Step : 50 , Loss: 1.1779\n",
            "Epoch : 42 , Step : 55 , Loss: 1.3638\n",
            "Epoch : 42 , Step : 60 , Loss: 1.3259\n",
            "Epoch : 43 , Step : 5 , Loss: 1.1781\n",
            "Epoch : 43 , Step : 10 , Loss: 1.3444\n",
            "Epoch : 43 , Step : 15 , Loss: 1.3108\n",
            "Epoch : 43 , Step : 20 , Loss: 1.2074\n",
            "Epoch : 43 , Step : 25 , Loss: 1.3357\n",
            "Epoch : 43 , Step : 30 , Loss: 1.2524\n",
            "Epoch : 43 , Step : 35 , Loss: 1.2619\n",
            "Epoch : 43 , Step : 40 , Loss: 1.2042\n",
            "Epoch : 43 , Step : 45 , Loss: 1.1689\n",
            "Epoch : 43 , Step : 50 , Loss: 1.1847\n",
            "Epoch : 43 , Step : 55 , Loss: 1.2139\n",
            "Epoch : 43 , Step : 60 , Loss: 1.3112\n",
            "Epoch : 44 , Step : 5 , Loss: 1.1725\n",
            "Epoch : 44 , Step : 10 , Loss: 1.2939\n",
            "Epoch : 44 , Step : 15 , Loss: 1.1549\n",
            "Epoch : 44 , Step : 20 , Loss: 1.3008\n",
            "Epoch : 44 , Step : 25 , Loss: 1.2529\n",
            "Epoch : 44 , Step : 30 , Loss: 1.1374\n",
            "Epoch : 44 , Step : 35 , Loss: 1.3180\n",
            "Epoch : 44 , Step : 40 , Loss: 1.2405\n",
            "Epoch : 44 , Step : 45 , Loss: 1.1878\n",
            "Epoch : 44 , Step : 50 , Loss: 1.1721\n",
            "Epoch : 44 , Step : 55 , Loss: 1.2482\n",
            "Epoch : 44 , Step : 60 , Loss: 1.4444\n",
            "Epoch : 45 , Step : 5 , Loss: 1.1384\n",
            "Epoch : 45 , Step : 10 , Loss: 1.2212\n",
            "Epoch : 45 , Step : 15 , Loss: 1.2621\n",
            "Epoch : 45 , Step : 20 , Loss: 1.1921\n",
            "Epoch : 45 , Step : 25 , Loss: 1.1695\n",
            "Epoch : 45 , Step : 30 , Loss: 1.1387\n",
            "Epoch : 45 , Step : 35 , Loss: 1.2256\n",
            "Epoch : 45 , Step : 40 , Loss: 1.2210\n",
            "Epoch : 45 , Step : 45 , Loss: 1.3214\n",
            "Epoch : 45 , Step : 50 , Loss: 1.1408\n",
            "Epoch : 45 , Step : 55 , Loss: 1.2144\n",
            "Epoch : 45 , Step : 60 , Loss: 1.1664\n",
            "Epoch : 46 , Step : 5 , Loss: 1.2458\n",
            "Epoch : 46 , Step : 10 , Loss: 1.2963\n",
            "Epoch : 46 , Step : 15 , Loss: 1.2725\n",
            "Epoch : 46 , Step : 20 , Loss: 1.1609\n",
            "Epoch : 46 , Step : 25 , Loss: 1.3491\n",
            "Epoch : 46 , Step : 30 , Loss: 1.2486\n",
            "Epoch : 46 , Step : 35 , Loss: 1.2051\n",
            "Epoch : 46 , Step : 40 , Loss: 1.2015\n",
            "Epoch : 46 , Step : 45 , Loss: 1.1768\n",
            "Epoch : 46 , Step : 50 , Loss: 1.2416\n",
            "Epoch : 46 , Step : 55 , Loss: 1.2177\n",
            "Epoch : 46 , Step : 60 , Loss: 1.2703\n",
            "Epoch : 47 , Step : 5 , Loss: 1.2646\n",
            "Epoch : 47 , Step : 10 , Loss: 1.1697\n",
            "Epoch : 47 , Step : 15 , Loss: 1.3071\n",
            "Epoch : 47 , Step : 20 , Loss: 1.4049\n",
            "Epoch : 47 , Step : 25 , Loss: 1.2246\n",
            "Epoch : 47 , Step : 30 , Loss: 1.2263\n",
            "Epoch : 47 , Step : 35 , Loss: 1.1994\n",
            "Epoch : 47 , Step : 40 , Loss: 1.2878\n",
            "Epoch : 47 , Step : 45 , Loss: 1.2923\n",
            "Epoch : 47 , Step : 50 , Loss: 1.1723\n",
            "Epoch : 47 , Step : 55 , Loss: 1.3341\n",
            "Epoch : 47 , Step : 60 , Loss: 1.5094\n",
            "Epoch : 48 , Step : 5 , Loss: 1.2211\n",
            "Epoch : 48 , Step : 10 , Loss: 1.1582\n",
            "Epoch : 48 , Step : 15 , Loss: 1.4819\n",
            "Epoch : 48 , Step : 20 , Loss: 1.3845\n",
            "Epoch : 48 , Step : 25 , Loss: 1.2792\n",
            "Epoch : 48 , Step : 30 , Loss: 1.1424\n",
            "Epoch : 48 , Step : 35 , Loss: 1.3236\n",
            "Epoch : 48 , Step : 40 , Loss: 1.2010\n",
            "Epoch : 48 , Step : 45 , Loss: 1.1646\n",
            "Epoch : 48 , Step : 50 , Loss: 1.2769\n",
            "Epoch : 48 , Step : 55 , Loss: 1.2894\n",
            "Epoch : 48 , Step : 60 , Loss: 1.2013\n",
            "Epoch : 49 , Step : 5 , Loss: 1.1692\n",
            "Epoch : 49 , Step : 10 , Loss: 1.3351\n",
            "Epoch : 49 , Step : 15 , Loss: 1.1591\n",
            "Epoch : 49 , Step : 20 , Loss: 1.2170\n",
            "Epoch : 49 , Step : 25 , Loss: 1.2814\n",
            "Epoch : 49 , Step : 30 , Loss: 1.1595\n",
            "Epoch : 49 , Step : 35 , Loss: 1.3472\n",
            "Epoch : 49 , Step : 40 , Loss: 1.1918\n",
            "Epoch : 49 , Step : 45 , Loss: 1.1909\n",
            "Epoch : 49 , Step : 50 , Loss: 1.2317\n",
            "Epoch : 49 , Step : 55 , Loss: 1.2248\n",
            "Epoch : 49 , Step : 60 , Loss: 1.2316\n",
            "Epoch : 50 , Step : 5 , Loss: 1.1580\n",
            "Epoch : 50 , Step : 10 , Loss: 1.3037\n",
            "Epoch : 50 , Step : 15 , Loss: 1.2278\n",
            "Epoch : 50 , Step : 20 , Loss: 1.2928\n",
            "Epoch : 50 , Step : 25 , Loss: 1.1816\n",
            "Epoch : 50 , Step : 30 , Loss: 1.1701\n",
            "Epoch : 50 , Step : 35 , Loss: 1.2354\n",
            "Epoch : 50 , Step : 40 , Loss: 1.1706\n",
            "Epoch : 50 , Step : 45 , Loss: 1.3483\n",
            "Epoch : 50 , Step : 50 , Loss: 1.1973\n",
            "Epoch : 50 , Step : 55 , Loss: 1.3322\n",
            "Epoch : 50 , Step : 60 , Loss: 1.3265\n",
            "Epoch : 51 , Step : 5 , Loss: 1.4383\n",
            "Epoch : 51 , Step : 10 , Loss: 1.2299\n",
            "Epoch : 51 , Step : 15 , Loss: 1.2380\n",
            "Epoch : 51 , Step : 20 , Loss: 1.3028\n",
            "Epoch : 51 , Step : 25 , Loss: 1.3086\n",
            "Epoch : 51 , Step : 30 , Loss: 1.3956\n",
            "Epoch : 51 , Step : 35 , Loss: 1.3974\n",
            "Epoch : 51 , Step : 40 , Loss: 1.3586\n",
            "Epoch : 51 , Step : 45 , Loss: 1.1915\n",
            "Epoch : 51 , Step : 50 , Loss: 1.2348\n",
            "Epoch : 51 , Step : 55 , Loss: 1.1888\n",
            "Epoch : 51 , Step : 60 , Loss: 1.3265\n",
            "Epoch : 52 , Step : 5 , Loss: 1.3135\n",
            "Epoch : 52 , Step : 10 , Loss: 1.2100\n",
            "Epoch : 52 , Step : 15 , Loss: 1.3015\n",
            "Epoch : 52 , Step : 20 , Loss: 1.2516\n",
            "Epoch : 52 , Step : 25 , Loss: 1.2888\n",
            "Epoch : 52 , Step : 30 , Loss: 1.4331\n",
            "Epoch : 52 , Step : 35 , Loss: 1.1792\n",
            "Epoch : 52 , Step : 40 , Loss: 1.2767\n",
            "Epoch : 52 , Step : 45 , Loss: 1.3679\n",
            "Epoch : 52 , Step : 50 , Loss: 1.2074\n",
            "Epoch : 52 , Step : 55 , Loss: 1.2922\n",
            "Epoch : 52 , Step : 60 , Loss: 1.3111\n",
            "Epoch : 53 , Step : 5 , Loss: 1.2004\n",
            "Epoch : 53 , Step : 10 , Loss: 1.1107\n",
            "Epoch : 53 , Step : 15 , Loss: 1.2741\n",
            "Epoch : 53 , Step : 20 , Loss: 1.2113\n",
            "Epoch : 53 , Step : 25 , Loss: 1.2771\n",
            "Epoch : 53 , Step : 30 , Loss: 1.1850\n",
            "Epoch : 53 , Step : 35 , Loss: 1.3745\n",
            "Epoch : 53 , Step : 40 , Loss: 1.2620\n",
            "Epoch : 53 , Step : 45 , Loss: 1.2166\n",
            "Epoch : 53 , Step : 50 , Loss: 1.2324\n",
            "Epoch : 53 , Step : 55 , Loss: 1.2311\n",
            "Epoch : 53 , Step : 60 , Loss: 1.2311\n",
            "Epoch : 54 , Step : 5 , Loss: 1.2941\n",
            "Epoch : 54 , Step : 10 , Loss: 1.2938\n",
            "Epoch : 54 , Step : 15 , Loss: 1.3039\n",
            "Epoch : 54 , Step : 20 , Loss: 1.3394\n",
            "Epoch : 54 , Step : 25 , Loss: 1.3841\n",
            "Epoch : 54 , Step : 30 , Loss: 1.1706\n",
            "Epoch : 54 , Step : 35 , Loss: 1.1147\n",
            "Epoch : 54 , Step : 40 , Loss: 1.1069\n",
            "Epoch : 54 , Step : 45 , Loss: 1.1998\n",
            "Epoch : 54 , Step : 50 , Loss: 1.3031\n",
            "Epoch : 54 , Step : 55 , Loss: 1.3166\n",
            "Epoch : 54 , Step : 60 , Loss: 1.2299\n",
            "Epoch : 55 , Step : 5 , Loss: 1.2682\n",
            "Epoch : 55 , Step : 10 , Loss: 1.2007\n",
            "Epoch : 55 , Step : 15 , Loss: 1.2416\n",
            "Epoch : 55 , Step : 20 , Loss: 1.1588\n",
            "Epoch : 55 , Step : 25 , Loss: 1.2291\n",
            "Epoch : 55 , Step : 30 , Loss: 1.2323\n",
            "Epoch : 55 , Step : 35 , Loss: 1.3394\n",
            "Epoch : 55 , Step : 40 , Loss: 1.1993\n",
            "Epoch : 55 , Step : 45 , Loss: 1.1912\n",
            "Epoch : 55 , Step : 50 , Loss: 1.2890\n",
            "Epoch : 55 , Step : 55 , Loss: 1.2397\n",
            "Epoch : 55 , Step : 60 , Loss: 1.1393\n",
            "Epoch : 56 , Step : 5 , Loss: 1.3041\n",
            "Epoch : 56 , Step : 10 , Loss: 1.3232\n",
            "Epoch : 56 , Step : 15 , Loss: 1.2821\n",
            "Epoch : 56 , Step : 20 , Loss: 1.3623\n",
            "Epoch : 56 , Step : 25 , Loss: 1.1792\n",
            "Epoch : 56 , Step : 30 , Loss: 1.1371\n",
            "Epoch : 56 , Step : 35 , Loss: 1.2219\n",
            "Epoch : 56 , Step : 40 , Loss: 1.2260\n",
            "Epoch : 56 , Step : 45 , Loss: 1.3037\n",
            "Epoch : 56 , Step : 50 , Loss: 1.2795\n",
            "Epoch : 56 , Step : 55 , Loss: 1.2172\n",
            "Epoch : 56 , Step : 60 , Loss: 1.2665\n",
            "Epoch : 57 , Step : 5 , Loss: 1.2378\n",
            "Epoch : 57 , Step : 10 , Loss: 1.2272\n",
            "Epoch : 57 , Step : 15 , Loss: 1.3219\n",
            "Epoch : 57 , Step : 20 , Loss: 1.2634\n",
            "Epoch : 57 , Step : 25 , Loss: 1.3108\n",
            "Epoch : 57 , Step : 30 , Loss: 1.3344\n",
            "Epoch : 57 , Step : 35 , Loss: 1.3542\n",
            "Epoch : 57 , Step : 40 , Loss: 1.1676\n",
            "Epoch : 57 , Step : 45 , Loss: 1.2311\n",
            "Epoch : 57 , Step : 50 , Loss: 1.3247\n",
            "Epoch : 57 , Step : 55 , Loss: 1.1995\n",
            "Epoch : 57 , Step : 60 , Loss: 1.1650\n",
            "Epoch : 58 , Step : 5 , Loss: 1.2430\n",
            "Epoch : 58 , Step : 10 , Loss: 1.2059\n",
            "Epoch : 58 , Step : 15 , Loss: 1.2115\n",
            "Epoch : 58 , Step : 20 , Loss: 1.2380\n",
            "Epoch : 58 , Step : 25 , Loss: 1.3265\n",
            "Epoch : 58 , Step : 30 , Loss: 1.1902\n",
            "Epoch : 58 , Step : 35 , Loss: 1.1673\n",
            "Epoch : 58 , Step : 40 , Loss: 1.2929\n",
            "Epoch : 58 , Step : 45 , Loss: 1.2331\n",
            "Epoch : 58 , Step : 50 , Loss: 1.3559\n",
            "Epoch : 58 , Step : 55 , Loss: 1.2555\n",
            "Epoch : 58 , Step : 60 , Loss: 1.2187\n",
            "Epoch : 59 , Step : 5 , Loss: 1.3143\n",
            "Epoch : 59 , Step : 10 , Loss: 1.1909\n",
            "Epoch : 59 , Step : 15 , Loss: 1.1688\n",
            "Epoch : 59 , Step : 20 , Loss: 1.1666\n",
            "Epoch : 59 , Step : 25 , Loss: 1.2611\n",
            "Epoch : 59 , Step : 30 , Loss: 1.2647\n",
            "Epoch : 59 , Step : 35 , Loss: 1.2279\n",
            "Epoch : 59 , Step : 40 , Loss: 1.1986\n",
            "Epoch : 59 , Step : 45 , Loss: 1.6333\n",
            "Epoch : 59 , Step : 50 , Loss: 1.3554\n",
            "Epoch : 59 , Step : 55 , Loss: 1.4546\n",
            "Epoch : 59 , Step : 60 , Loss: 1.1061\n",
            "Epoch : 60 , Step : 5 , Loss: 1.3245\n",
            "Epoch : 60 , Step : 10 , Loss: 1.1694\n",
            "Epoch : 60 , Step : 15 , Loss: 1.2182\n",
            "Epoch : 60 , Step : 20 , Loss: 1.3754\n",
            "Epoch : 60 , Step : 25 , Loss: 1.2352\n",
            "Epoch : 60 , Step : 30 , Loss: 1.3493\n",
            "Epoch : 60 , Step : 35 , Loss: 1.1411\n",
            "Epoch : 60 , Step : 40 , Loss: 1.3349\n",
            "Epoch : 60 , Step : 45 , Loss: 1.2519\n",
            "Epoch : 60 , Step : 50 , Loss: 1.2110\n",
            "Epoch : 60 , Step : 55 , Loss: 1.2025\n",
            "Epoch : 60 , Step : 60 , Loss: 1.2586\n",
            "Epoch : 61 , Step : 5 , Loss: 1.1976\n",
            "Epoch : 61 , Step : 10 , Loss: 1.2926\n",
            "Epoch : 61 , Step : 15 , Loss: 1.1238\n",
            "Epoch : 61 , Step : 20 , Loss: 1.1379\n",
            "Epoch : 61 , Step : 25 , Loss: 1.4137\n",
            "Epoch : 61 , Step : 30 , Loss: 1.1312\n",
            "Epoch : 61 , Step : 35 , Loss: 1.2874\n",
            "Epoch : 61 , Step : 40 , Loss: 1.2611\n",
            "Epoch : 61 , Step : 45 , Loss: 1.1719\n",
            "Epoch : 61 , Step : 50 , Loss: 1.2724\n",
            "Epoch : 61 , Step : 55 , Loss: 1.1800\n",
            "Epoch : 61 , Step : 60 , Loss: 1.1352\n",
            "Epoch : 62 , Step : 5 , Loss: 1.2602\n",
            "Epoch : 62 , Step : 10 , Loss: 1.2608\n",
            "Epoch : 62 , Step : 15 , Loss: 1.3484\n",
            "Epoch : 62 , Step : 20 , Loss: 1.1885\n",
            "Epoch : 62 , Step : 25 , Loss: 1.1711\n",
            "Epoch : 62 , Step : 30 , Loss: 1.3547\n",
            "Epoch : 62 , Step : 35 , Loss: 1.3469\n",
            "Epoch : 62 , Step : 40 , Loss: 1.1998\n",
            "Epoch : 62 , Step : 45 , Loss: 1.3173\n",
            "Epoch : 62 , Step : 50 , Loss: 1.2220\n",
            "Epoch : 62 , Step : 55 , Loss: 1.2884\n",
            "Epoch : 62 , Step : 60 , Loss: 1.2648\n",
            "Epoch : 63 , Step : 5 , Loss: 1.2808\n",
            "Epoch : 63 , Step : 10 , Loss: 1.2802\n",
            "Epoch : 63 , Step : 15 , Loss: 1.2265\n",
            "Epoch : 63 , Step : 20 , Loss: 1.2391\n",
            "Epoch : 63 , Step : 25 , Loss: 1.2396\n",
            "Epoch : 63 , Step : 30 , Loss: 1.2594\n",
            "Epoch : 63 , Step : 35 , Loss: 1.2617\n",
            "Epoch : 63 , Step : 40 , Loss: 1.1176\n",
            "Epoch : 63 , Step : 45 , Loss: 1.1588\n",
            "Epoch : 63 , Step : 50 , Loss: 1.1964\n",
            "Epoch : 63 , Step : 55 , Loss: 1.1095\n",
            "Epoch : 63 , Step : 60 , Loss: 1.3852\n",
            "Epoch : 64 , Step : 5 , Loss: 1.3199\n",
            "Epoch : 64 , Step : 10 , Loss: 1.3396\n",
            "Epoch : 64 , Step : 15 , Loss: 1.2988\n",
            "Epoch : 64 , Step : 20 , Loss: 1.1546\n",
            "Epoch : 64 , Step : 25 , Loss: 1.2814\n",
            "Epoch : 64 , Step : 30 , Loss: 1.4053\n",
            "Epoch : 64 , Step : 35 , Loss: 1.1343\n",
            "Epoch : 64 , Step : 40 , Loss: 1.3436\n",
            "Epoch : 64 , Step : 45 , Loss: 1.2455\n",
            "Epoch : 64 , Step : 50 , Loss: 1.2538\n",
            "Epoch : 64 , Step : 55 , Loss: 1.1191\n",
            "Epoch : 64 , Step : 60 , Loss: 1.1968\n",
            "Epoch : 65 , Step : 5 , Loss: 1.3088\n",
            "Epoch : 65 , Step : 10 , Loss: 1.2424\n",
            "Epoch : 65 , Step : 15 , Loss: 1.2834\n",
            "Epoch : 65 , Step : 20 , Loss: 1.0875\n",
            "Epoch : 65 , Step : 25 , Loss: 1.2590\n",
            "Epoch : 65 , Step : 30 , Loss: 1.2329\n",
            "Epoch : 65 , Step : 35 , Loss: 1.2858\n",
            "Epoch : 65 , Step : 40 , Loss: 1.3471\n",
            "Epoch : 65 , Step : 45 , Loss: 1.2197\n",
            "Epoch : 65 , Step : 50 , Loss: 1.1684\n",
            "Epoch : 65 , Step : 55 , Loss: 1.3042\n",
            "Epoch : 65 , Step : 60 , Loss: 1.2620\n",
            "Epoch : 66 , Step : 5 , Loss: 1.2202\n",
            "Epoch : 66 , Step : 10 , Loss: 1.1259\n",
            "Epoch : 66 , Step : 15 , Loss: 1.1161\n",
            "Epoch : 66 , Step : 20 , Loss: 1.2191\n",
            "Epoch : 66 , Step : 25 , Loss: 1.3636\n",
            "Epoch : 66 , Step : 30 , Loss: 1.2630\n",
            "Epoch : 66 , Step : 35 , Loss: 1.2678\n",
            "Epoch : 66 , Step : 40 , Loss: 1.1756\n",
            "Epoch : 66 , Step : 45 , Loss: 1.1808\n",
            "Epoch : 66 , Step : 50 , Loss: 1.2569\n",
            "Epoch : 66 , Step : 55 , Loss: 1.1737\n",
            "Epoch : 66 , Step : 60 , Loss: 1.1939\n",
            "Epoch : 67 , Step : 5 , Loss: 1.3545\n",
            "Epoch : 67 , Step : 10 , Loss: 1.3202\n",
            "Epoch : 67 , Step : 15 , Loss: 1.2298\n",
            "Epoch : 67 , Step : 20 , Loss: 1.2111\n",
            "Epoch : 67 , Step : 25 , Loss: 1.1381\n",
            "Epoch : 67 , Step : 30 , Loss: 1.2278\n",
            "Epoch : 67 , Step : 35 , Loss: 1.3021\n",
            "Epoch : 67 , Step : 40 , Loss: 1.3326\n",
            "Epoch : 67 , Step : 45 , Loss: 1.1114\n",
            "Epoch : 67 , Step : 50 , Loss: 1.2000\n",
            "Epoch : 67 , Step : 55 , Loss: 1.2718\n",
            "Epoch : 67 , Step : 60 , Loss: 1.1843\n",
            "Epoch : 68 , Step : 5 , Loss: 1.1380\n",
            "Epoch : 68 , Step : 10 , Loss: 1.3837\n",
            "Epoch : 68 , Step : 15 , Loss: 1.2433\n",
            "Epoch : 68 , Step : 20 , Loss: 1.2312\n",
            "Epoch : 68 , Step : 25 , Loss: 1.0770\n",
            "Epoch : 68 , Step : 30 , Loss: 1.2746\n",
            "Epoch : 68 , Step : 35 , Loss: 1.2303\n",
            "Epoch : 68 , Step : 40 , Loss: 1.2060\n",
            "Epoch : 68 , Step : 45 , Loss: 1.1386\n",
            "Epoch : 68 , Step : 50 , Loss: 1.3413\n",
            "Epoch : 68 , Step : 55 , Loss: 1.3178\n",
            "Epoch : 68 , Step : 60 , Loss: 1.3786\n",
            "Epoch : 69 , Step : 5 , Loss: 1.2288\n",
            "Epoch : 69 , Step : 10 , Loss: 1.1698\n",
            "Epoch : 69 , Step : 15 , Loss: 1.2987\n",
            "Epoch : 69 , Step : 20 , Loss: 1.2025\n",
            "Epoch : 69 , Step : 25 , Loss: 1.2876\n",
            "Epoch : 69 , Step : 30 , Loss: 1.2019\n",
            "Epoch : 69 , Step : 35 , Loss: 1.2013\n",
            "Epoch : 69 , Step : 40 , Loss: 1.1998\n",
            "Epoch : 69 , Step : 45 , Loss: 1.1983\n",
            "Epoch : 69 , Step : 50 , Loss: 1.2910\n",
            "Epoch : 69 , Step : 55 , Loss: 1.2980\n",
            "Epoch : 69 , Step : 60 , Loss: 1.1444\n",
            "Epoch : 70 , Step : 5 , Loss: 1.1377\n",
            "Epoch : 70 , Step : 10 , Loss: 1.3136\n",
            "Epoch : 70 , Step : 15 , Loss: 1.1120\n",
            "Epoch : 70 , Step : 20 , Loss: 1.3559\n",
            "Epoch : 70 , Step : 25 , Loss: 1.3185\n",
            "Epoch : 70 , Step : 30 , Loss: 1.2195\n",
            "Epoch : 70 , Step : 35 , Loss: 1.2395\n",
            "Epoch : 70 , Step : 40 , Loss: 1.4166\n",
            "Epoch : 70 , Step : 45 , Loss: 1.2521\n",
            "Epoch : 70 , Step : 50 , Loss: 1.1546\n",
            "Epoch : 70 , Step : 55 , Loss: 1.3476\n",
            "Epoch : 70 , Step : 60 , Loss: 1.2122\n",
            "Epoch : 71 , Step : 5 , Loss: 1.2503\n",
            "Epoch : 71 , Step : 10 , Loss: 1.2189\n",
            "Epoch : 71 , Step : 15 , Loss: 1.2903\n",
            "Epoch : 71 , Step : 20 , Loss: 1.2019\n",
            "Epoch : 71 , Step : 25 , Loss: 1.1768\n",
            "Epoch : 71 , Step : 30 , Loss: 1.1712\n",
            "Epoch : 71 , Step : 35 , Loss: 1.1632\n",
            "Epoch : 71 , Step : 40 , Loss: 1.2001\n",
            "Epoch : 71 , Step : 45 , Loss: 1.3703\n",
            "Epoch : 71 , Step : 50 , Loss: 1.1826\n",
            "Epoch : 71 , Step : 55 , Loss: 1.2514\n",
            "Epoch : 71 , Step : 60 , Loss: 1.2598\n",
            "Epoch : 72 , Step : 5 , Loss: 1.2295\n",
            "Epoch : 72 , Step : 10 , Loss: 1.2090\n",
            "Epoch : 72 , Step : 15 , Loss: 1.1521\n",
            "Epoch : 72 , Step : 20 , Loss: 1.1765\n",
            "Epoch : 72 , Step : 25 , Loss: 1.2934\n",
            "Epoch : 72 , Step : 30 , Loss: 1.2709\n",
            "Epoch : 72 , Step : 35 , Loss: 1.1395\n",
            "Epoch : 72 , Step : 40 , Loss: 1.2339\n",
            "Epoch : 72 , Step : 45 , Loss: 1.2961\n",
            "Epoch : 72 , Step : 50 , Loss: 1.1046\n",
            "Epoch : 72 , Step : 55 , Loss: 1.1678\n",
            "Epoch : 72 , Step : 60 , Loss: 1.2363\n",
            "Epoch : 73 , Step : 5 , Loss: 1.3112\n",
            "Epoch : 73 , Step : 10 , Loss: 1.2308\n",
            "Epoch : 73 , Step : 15 , Loss: 1.2219\n",
            "Epoch : 73 , Step : 20 , Loss: 1.3238\n",
            "Epoch : 73 , Step : 25 , Loss: 1.2009\n",
            "Epoch : 73 , Step : 30 , Loss: 1.1738\n",
            "Epoch : 73 , Step : 35 , Loss: 1.2145\n",
            "Epoch : 73 , Step : 40 , Loss: 1.2151\n",
            "Epoch : 73 , Step : 45 , Loss: 1.2648\n",
            "Epoch : 73 , Step : 50 , Loss: 1.3436\n",
            "Epoch : 73 , Step : 55 , Loss: 1.2765\n",
            "Epoch : 73 , Step : 60 , Loss: 1.1987\n",
            "Epoch : 74 , Step : 5 , Loss: 1.1740\n",
            "Epoch : 74 , Step : 10 , Loss: 1.4479\n",
            "Epoch : 74 , Step : 15 , Loss: 1.2483\n",
            "Epoch : 74 , Step : 20 , Loss: 1.2310\n",
            "Epoch : 74 , Step : 25 , Loss: 1.2259\n",
            "Epoch : 74 , Step : 30 , Loss: 1.2616\n",
            "Epoch : 74 , Step : 35 , Loss: 1.3254\n",
            "Epoch : 74 , Step : 40 , Loss: 1.1689\n",
            "Epoch : 74 , Step : 45 , Loss: 1.2111\n",
            "Epoch : 74 , Step : 50 , Loss: 1.2957\n",
            "Epoch : 74 , Step : 55 , Loss: 1.2134\n",
            "Epoch : 74 , Step : 60 , Loss: 1.2161\n",
            "Epoch : 75 , Step : 5 , Loss: 1.2030\n",
            "Epoch : 75 , Step : 10 , Loss: 1.2542\n",
            "Epoch : 75 , Step : 15 , Loss: 1.1988\n",
            "Epoch : 75 , Step : 20 , Loss: 1.2295\n",
            "Epoch : 75 , Step : 25 , Loss: 1.2807\n",
            "Epoch : 75 , Step : 30 , Loss: 1.2477\n",
            "Epoch : 75 , Step : 35 , Loss: 1.2050\n",
            "Epoch : 75 , Step : 40 , Loss: 1.2483\n",
            "Epoch : 75 , Step : 45 , Loss: 1.1676\n",
            "Epoch : 75 , Step : 50 , Loss: 1.2256\n",
            "Epoch : 75 , Step : 55 , Loss: 1.0732\n",
            "Epoch : 75 , Step : 60 , Loss: 1.1450\n",
            "Epoch : 76 , Step : 5 , Loss: 1.1687\n",
            "Epoch : 76 , Step : 10 , Loss: 1.2919\n",
            "Epoch : 76 , Step : 15 , Loss: 1.1063\n",
            "Epoch : 76 , Step : 20 , Loss: 1.1161\n",
            "Epoch : 76 , Step : 25 , Loss: 1.2066\n",
            "Epoch : 76 , Step : 30 , Loss: 1.2028\n",
            "Epoch : 76 , Step : 35 , Loss: 1.2229\n",
            "Epoch : 76 , Step : 40 , Loss: 1.2612\n",
            "Epoch : 76 , Step : 45 , Loss: 1.1683\n",
            "Epoch : 76 , Step : 50 , Loss: 1.2665\n",
            "Epoch : 76 , Step : 55 , Loss: 1.3018\n",
            "Epoch : 76 , Step : 60 , Loss: 1.3586\n",
            "Epoch : 77 , Step : 5 , Loss: 1.2447\n",
            "Epoch : 77 , Step : 10 , Loss: 1.2308\n",
            "Epoch : 77 , Step : 15 , Loss: 1.3177\n",
            "Epoch : 77 , Step : 20 , Loss: 1.0791\n",
            "Epoch : 77 , Step : 25 , Loss: 1.2319\n",
            "Epoch : 77 , Step : 30 , Loss: 1.1938\n",
            "Epoch : 77 , Step : 35 , Loss: 1.1965\n",
            "Epoch : 77 , Step : 40 , Loss: 1.1944\n",
            "Epoch : 77 , Step : 45 , Loss: 1.1497\n",
            "Epoch : 77 , Step : 50 , Loss: 1.1872\n",
            "Epoch : 77 , Step : 55 , Loss: 1.3144\n",
            "Epoch : 77 , Step : 60 , Loss: 1.2937\n",
            "Epoch : 78 , Step : 5 , Loss: 1.1723\n",
            "Epoch : 78 , Step : 10 , Loss: 1.2555\n",
            "Epoch : 78 , Step : 15 , Loss: 1.2676\n",
            "Epoch : 78 , Step : 20 , Loss: 1.4014\n",
            "Epoch : 78 , Step : 25 , Loss: 1.2283\n",
            "Epoch : 78 , Step : 30 , Loss: 1.3748\n",
            "Epoch : 78 , Step : 35 , Loss: 1.2507\n",
            "Epoch : 78 , Step : 40 , Loss: 1.1597\n",
            "Epoch : 78 , Step : 45 , Loss: 1.2483\n",
            "Epoch : 78 , Step : 50 , Loss: 1.1475\n",
            "Epoch : 78 , Step : 55 , Loss: 1.2311\n",
            "Epoch : 78 , Step : 60 , Loss: 1.3488\n",
            "Epoch : 79 , Step : 5 , Loss: 1.2392\n",
            "Epoch : 79 , Step : 10 , Loss: 1.2927\n",
            "Epoch : 79 , Step : 15 , Loss: 1.2001\n",
            "Epoch : 79 , Step : 20 , Loss: 1.1688\n",
            "Epoch : 79 , Step : 25 , Loss: 1.3247\n",
            "Epoch : 79 , Step : 30 , Loss: 1.2330\n",
            "Epoch : 79 , Step : 35 , Loss: 1.2311\n",
            "Epoch : 79 , Step : 40 , Loss: 1.1109\n",
            "Epoch : 79 , Step : 45 , Loss: 1.1973\n",
            "Epoch : 79 , Step : 50 , Loss: 1.2618\n",
            "Epoch : 79 , Step : 55 , Loss: 1.2000\n",
            "Epoch : 79 , Step : 60 , Loss: 1.2700\n",
            "Epoch : 80 , Step : 5 , Loss: 1.3576\n",
            "Epoch : 80 , Step : 10 , Loss: 1.2315\n",
            "Epoch : 80 , Step : 15 , Loss: 1.1937\n",
            "Epoch : 80 , Step : 20 , Loss: 1.2134\n",
            "Epoch : 80 , Step : 25 , Loss: 1.2551\n",
            "Epoch : 80 , Step : 30 , Loss: 1.2090\n",
            "Epoch : 80 , Step : 35 , Loss: 1.2574\n",
            "Epoch : 80 , Step : 40 , Loss: 1.2614\n",
            "Epoch : 80 , Step : 45 , Loss: 1.1985\n",
            "Epoch : 80 , Step : 50 , Loss: 1.2296\n",
            "Epoch : 80 , Step : 55 , Loss: 1.1053\n",
            "Epoch : 80 , Step : 60 , Loss: 1.2709\n",
            "Epoch : 81 , Step : 5 , Loss: 1.1518\n",
            "Epoch : 81 , Step : 10 , Loss: 1.1685\n",
            "Epoch : 81 , Step : 15 , Loss: 1.2626\n",
            "Epoch : 81 , Step : 20 , Loss: 1.2029\n",
            "Epoch : 81 , Step : 25 , Loss: 1.2277\n",
            "Epoch : 81 , Step : 30 , Loss: 1.2401\n",
            "Epoch : 81 , Step : 35 , Loss: 1.2724\n",
            "Epoch : 81 , Step : 40 , Loss: 1.1982\n",
            "Epoch : 81 , Step : 45 , Loss: 1.2116\n",
            "Epoch : 81 , Step : 50 , Loss: 1.1985\n",
            "Epoch : 81 , Step : 55 , Loss: 1.3534\n",
            "Epoch : 81 , Step : 60 , Loss: 1.2277\n",
            "Epoch : 82 , Step : 5 , Loss: 1.3072\n",
            "Epoch : 82 , Step : 10 , Loss: 1.2298\n",
            "Epoch : 82 , Step : 15 , Loss: 1.1994\n",
            "Epoch : 82 , Step : 20 , Loss: 1.1686\n",
            "Epoch : 82 , Step : 25 , Loss: 1.2343\n",
            "Epoch : 82 , Step : 30 , Loss: 1.2453\n",
            "Epoch : 82 , Step : 35 , Loss: 1.2010\n",
            "Epoch : 82 , Step : 40 , Loss: 1.3280\n",
            "Epoch : 82 , Step : 45 , Loss: 1.4029\n",
            "Epoch : 82 , Step : 50 , Loss: 1.3538\n",
            "Epoch : 82 , Step : 55 , Loss: 1.1329\n",
            "Epoch : 82 , Step : 60 , Loss: 1.1725\n",
            "Epoch : 83 , Step : 5 , Loss: 1.3279\n",
            "Epoch : 83 , Step : 10 , Loss: 1.2287\n",
            "Epoch : 83 , Step : 15 , Loss: 1.1951\n",
            "Epoch : 83 , Step : 20 , Loss: 1.2903\n",
            "Epoch : 83 , Step : 25 , Loss: 1.0439\n",
            "Epoch : 83 , Step : 30 , Loss: 1.2283\n",
            "Epoch : 83 , Step : 35 , Loss: 1.1373\n",
            "Epoch : 83 , Step : 40 , Loss: 1.2696\n",
            "Epoch : 83 , Step : 45 , Loss: 1.2055\n",
            "Epoch : 83 , Step : 50 , Loss: 1.2002\n",
            "Epoch : 83 , Step : 55 , Loss: 1.2829\n",
            "Epoch : 83 , Step : 60 , Loss: 1.2076\n",
            "Epoch : 84 , Step : 5 , Loss: 1.2058\n",
            "Epoch : 84 , Step : 10 , Loss: 1.1817\n",
            "Epoch : 84 , Step : 15 , Loss: 1.3217\n",
            "Epoch : 84 , Step : 20 , Loss: 1.2220\n",
            "Epoch : 84 , Step : 25 , Loss: 1.1205\n",
            "Epoch : 84 , Step : 30 , Loss: 1.2370\n",
            "Epoch : 84 , Step : 35 , Loss: 1.1396\n",
            "Epoch : 84 , Step : 40 , Loss: 1.2649\n",
            "Epoch : 84 , Step : 45 , Loss: 1.2617\n",
            "Epoch : 84 , Step : 50 , Loss: 1.3393\n",
            "Epoch : 84 , Step : 55 , Loss: 1.3971\n",
            "Epoch : 84 , Step : 60 , Loss: 1.2929\n",
            "Epoch : 85 , Step : 5 , Loss: 1.0936\n",
            "Epoch : 85 , Step : 10 , Loss: 1.3160\n",
            "Epoch : 85 , Step : 15 , Loss: 1.1677\n",
            "Epoch : 85 , Step : 20 , Loss: 1.2278\n",
            "Epoch : 85 , Step : 25 , Loss: 1.2976\n",
            "Epoch : 85 , Step : 30 , Loss: 1.3421\n",
            "Epoch : 85 , Step : 35 , Loss: 1.2314\n",
            "Epoch : 85 , Step : 40 , Loss: 1.1079\n",
            "Epoch : 85 , Step : 45 , Loss: 1.2331\n",
            "Epoch : 85 , Step : 50 , Loss: 1.1496\n",
            "Epoch : 85 , Step : 55 , Loss: 1.1060\n",
            "Epoch : 85 , Step : 60 , Loss: 1.1502\n",
            "Epoch : 86 , Step : 5 , Loss: 1.2513\n",
            "Epoch : 86 , Step : 10 , Loss: 1.1906\n",
            "Epoch : 86 , Step : 15 , Loss: 1.1996\n",
            "Epoch : 86 , Step : 20 , Loss: 1.3062\n",
            "Epoch : 86 , Step : 25 , Loss: 1.2891\n",
            "Epoch : 86 , Step : 30 , Loss: 1.1998\n",
            "Epoch : 86 , Step : 35 , Loss: 1.1668\n",
            "Epoch : 86 , Step : 40 , Loss: 1.2997\n",
            "Epoch : 86 , Step : 45 , Loss: 1.2145\n",
            "Epoch : 86 , Step : 50 , Loss: 1.1553\n",
            "Epoch : 86 , Step : 55 , Loss: 1.3487\n",
            "Epoch : 86 , Step : 60 , Loss: 1.2171\n",
            "Epoch : 87 , Step : 5 , Loss: 1.2661\n",
            "Epoch : 87 , Step : 10 , Loss: 1.2411\n",
            "Epoch : 87 , Step : 15 , Loss: 1.2226\n",
            "Epoch : 87 , Step : 20 , Loss: 1.2883\n",
            "Epoch : 87 , Step : 25 , Loss: 1.1970\n",
            "Epoch : 87 , Step : 30 , Loss: 1.2504\n",
            "Epoch : 87 , Step : 35 , Loss: 1.1875\n",
            "Epoch : 87 , Step : 40 , Loss: 1.3339\n",
            "Epoch : 87 , Step : 45 , Loss: 1.2912\n",
            "Epoch : 87 , Step : 50 , Loss: 1.2303\n",
            "Epoch : 87 , Step : 55 , Loss: 1.2379\n",
            "Epoch : 87 , Step : 60 , Loss: 1.2093\n",
            "Epoch : 88 , Step : 5 , Loss: 1.1342\n",
            "Epoch : 88 , Step : 10 , Loss: 1.2931\n",
            "Epoch : 88 , Step : 15 , Loss: 1.2671\n",
            "Epoch : 88 , Step : 20 , Loss: 1.1968\n",
            "Epoch : 88 , Step : 25 , Loss: 1.3009\n",
            "Epoch : 88 , Step : 30 , Loss: 1.2881\n",
            "Epoch : 88 , Step : 35 , Loss: 1.3227\n",
            "Epoch : 88 , Step : 40 , Loss: 1.3125\n",
            "Epoch : 88 , Step : 45 , Loss: 1.1622\n",
            "Epoch : 88 , Step : 50 , Loss: 1.2635\n",
            "Epoch : 88 , Step : 55 , Loss: 1.2071\n",
            "Epoch : 88 , Step : 60 , Loss: 1.2980\n",
            "Epoch : 89 , Step : 5 , Loss: 1.3207\n",
            "Epoch : 89 , Step : 10 , Loss: 1.2476\n",
            "Epoch : 89 , Step : 15 , Loss: 1.2318\n",
            "Epoch : 89 , Step : 20 , Loss: 1.1961\n",
            "Epoch : 89 , Step : 25 , Loss: 1.2288\n",
            "Epoch : 89 , Step : 30 , Loss: 1.2599\n",
            "Epoch : 89 , Step : 35 , Loss: 1.1685\n",
            "Epoch : 89 , Step : 40 , Loss: 1.2593\n",
            "Epoch : 89 , Step : 45 , Loss: 1.2291\n",
            "Epoch : 89 , Step : 50 , Loss: 1.2307\n",
            "Epoch : 89 , Step : 55 , Loss: 1.2008\n",
            "Epoch : 89 , Step : 60 , Loss: 1.2595\n",
            "Epoch : 90 , Step : 5 , Loss: 1.1351\n",
            "Epoch : 90 , Step : 10 , Loss: 1.3372\n",
            "Epoch : 90 , Step : 15 , Loss: 1.2093\n",
            "Epoch : 90 , Step : 20 , Loss: 1.2593\n",
            "Epoch : 90 , Step : 25 , Loss: 1.1704\n",
            "Epoch : 90 , Step : 30 , Loss: 1.2070\n",
            "Epoch : 90 , Step : 35 , Loss: 1.3676\n",
            "Epoch : 90 , Step : 40 , Loss: 1.1044\n",
            "Epoch : 90 , Step : 45 , Loss: 1.1690\n",
            "Epoch : 90 , Step : 50 , Loss: 1.2024\n",
            "Epoch : 90 , Step : 55 , Loss: 1.1634\n",
            "Epoch : 90 , Step : 60 , Loss: 1.1726\n",
            "Epoch : 91 , Step : 5 , Loss: 1.3231\n",
            "Epoch : 91 , Step : 10 , Loss: 1.2694\n",
            "Epoch : 91 , Step : 15 , Loss: 1.1628\n",
            "Epoch : 91 , Step : 20 , Loss: 1.3606\n",
            "Epoch : 91 , Step : 25 , Loss: 1.1465\n",
            "Epoch : 91 , Step : 30 , Loss: 1.2679\n",
            "Epoch : 91 , Step : 35 , Loss: 1.2934\n",
            "Epoch : 91 , Step : 40 , Loss: 1.2120\n",
            "Epoch : 91 , Step : 45 , Loss: 1.1547\n",
            "Epoch : 91 , Step : 50 , Loss: 1.1164\n",
            "Epoch : 91 , Step : 55 , Loss: 1.2521\n",
            "Epoch : 91 , Step : 60 , Loss: 1.1502\n",
            "Epoch : 92 , Step : 5 , Loss: 1.2010\n",
            "Epoch : 92 , Step : 10 , Loss: 1.1380\n",
            "Epoch : 92 , Step : 15 , Loss: 1.1998\n",
            "Epoch : 92 , Step : 20 , Loss: 1.2593\n",
            "Epoch : 92 , Step : 25 , Loss: 1.1333\n",
            "Epoch : 92 , Step : 30 , Loss: 1.2336\n",
            "Epoch : 92 , Step : 35 , Loss: 1.2604\n",
            "Epoch : 92 , Step : 40 , Loss: 1.1710\n",
            "Epoch : 92 , Step : 45 , Loss: 1.1455\n",
            "Epoch : 92 , Step : 50 , Loss: 1.1235\n",
            "Epoch : 92 , Step : 55 , Loss: 1.3334\n",
            "Epoch : 92 , Step : 60 , Loss: 1.2541\n",
            "Epoch : 93 , Step : 5 , Loss: 1.1409\n",
            "Epoch : 93 , Step : 10 , Loss: 1.2616\n",
            "Epoch : 93 , Step : 15 , Loss: 1.2678\n",
            "Epoch : 93 , Step : 20 , Loss: 1.3248\n",
            "Epoch : 93 , Step : 25 , Loss: 1.3147\n",
            "Epoch : 93 , Step : 30 , Loss: 1.2575\n",
            "Epoch : 93 , Step : 35 , Loss: 1.2293\n",
            "Epoch : 93 , Step : 40 , Loss: 1.3423\n",
            "Epoch : 93 , Step : 45 , Loss: 1.2016\n",
            "Epoch : 93 , Step : 50 , Loss: 1.1028\n",
            "Epoch : 93 , Step : 55 , Loss: 1.1447\n",
            "Epoch : 93 , Step : 60 , Loss: 1.1684\n",
            "Epoch : 94 , Step : 5 , Loss: 1.2000\n",
            "Epoch : 94 , Step : 10 , Loss: 1.2289\n",
            "Epoch : 94 , Step : 15 , Loss: 1.2935\n",
            "Epoch : 94 , Step : 20 , Loss: 1.1956\n",
            "Epoch : 94 , Step : 25 , Loss: 1.2265\n",
            "Epoch : 94 , Step : 30 , Loss: 1.0816\n",
            "Epoch : 94 , Step : 35 , Loss: 1.2254\n",
            "Epoch : 94 , Step : 40 , Loss: 1.2120\n",
            "Epoch : 94 , Step : 45 , Loss: 1.1685\n",
            "Epoch : 94 , Step : 50 , Loss: 1.3764\n",
            "Epoch : 94 , Step : 55 , Loss: 1.2876\n",
            "Epoch : 94 , Step : 60 , Loss: 1.2648\n",
            "Epoch : 95 , Step : 5 , Loss: 1.2211\n",
            "Epoch : 95 , Step : 10 , Loss: 1.1724\n",
            "Epoch : 95 , Step : 15 , Loss: 1.1740\n",
            "Epoch : 95 , Step : 20 , Loss: 1.1768\n",
            "Epoch : 95 , Step : 25 , Loss: 1.2004\n",
            "Epoch : 95 , Step : 30 , Loss: 1.2926\n",
            "Epoch : 95 , Step : 35 , Loss: 1.2044\n",
            "Epoch : 95 , Step : 40 , Loss: 1.2000\n",
            "Epoch : 95 , Step : 45 , Loss: 1.2189\n",
            "Epoch : 95 , Step : 50 , Loss: 1.2032\n",
            "Epoch : 95 , Step : 55 , Loss: 1.1918\n",
            "Epoch : 95 , Step : 60 , Loss: 1.3220\n",
            "Epoch : 96 , Step : 5 , Loss: 1.1674\n",
            "Epoch : 96 , Step : 10 , Loss: 1.1874\n",
            "Epoch : 96 , Step : 15 , Loss: 1.1200\n",
            "Epoch : 96 , Step : 20 , Loss: 1.1320\n",
            "Epoch : 96 , Step : 25 , Loss: 1.2586\n",
            "Epoch : 96 , Step : 30 , Loss: 1.2265\n",
            "Epoch : 96 , Step : 35 , Loss: 1.1997\n",
            "Epoch : 96 , Step : 40 , Loss: 1.2357\n",
            "Epoch : 96 , Step : 45 , Loss: 1.3151\n",
            "Epoch : 96 , Step : 50 , Loss: 1.2545\n",
            "Epoch : 96 , Step : 55 , Loss: 1.3224\n",
            "Epoch : 96 , Step : 60 , Loss: 1.2440\n",
            "Epoch : 97 , Step : 5 , Loss: 1.2887\n",
            "Epoch : 97 , Step : 10 , Loss: 1.1928\n",
            "Epoch : 97 , Step : 15 , Loss: 1.2593\n",
            "Epoch : 97 , Step : 20 , Loss: 1.2230\n",
            "Epoch : 97 , Step : 25 , Loss: 1.2490\n",
            "Epoch : 97 , Step : 30 , Loss: 1.1689\n",
            "Epoch : 97 , Step : 35 , Loss: 1.1682\n",
            "Epoch : 97 , Step : 40 , Loss: 1.2565\n",
            "Epoch : 97 , Step : 45 , Loss: 1.2920\n",
            "Epoch : 97 , Step : 50 , Loss: 1.1642\n",
            "Epoch : 97 , Step : 55 , Loss: 1.1492\n",
            "Epoch : 97 , Step : 60 , Loss: 1.2602\n",
            "Epoch : 98 , Step : 5 , Loss: 1.2833\n",
            "Epoch : 98 , Step : 10 , Loss: 1.2910\n",
            "Epoch : 98 , Step : 15 , Loss: 1.2596\n",
            "Epoch : 98 , Step : 20 , Loss: 1.2758\n",
            "Epoch : 98 , Step : 25 , Loss: 1.2305\n",
            "Epoch : 98 , Step : 30 , Loss: 1.1066\n",
            "Epoch : 98 , Step : 35 , Loss: 1.2602\n",
            "Epoch : 98 , Step : 40 , Loss: 1.2365\n",
            "Epoch : 98 , Step : 45 , Loss: 1.2823\n",
            "Epoch : 98 , Step : 50 , Loss: 1.1733\n",
            "Epoch : 98 , Step : 55 , Loss: 1.2401\n",
            "Epoch : 98 , Step : 60 , Loss: 1.1978\n",
            "Epoch : 99 , Step : 5 , Loss: 1.2312\n",
            "Epoch : 99 , Step : 10 , Loss: 1.2820\n",
            "Epoch : 99 , Step : 15 , Loss: 1.1669\n",
            "Epoch : 99 , Step : 20 , Loss: 1.1276\n",
            "Epoch : 99 , Step : 25 , Loss: 1.3534\n",
            "Epoch : 99 , Step : 30 , Loss: 1.2315\n",
            "Epoch : 99 , Step : 35 , Loss: 1.2365\n",
            "Epoch : 99 , Step : 40 , Loss: 1.1354\n",
            "Epoch : 99 , Step : 45 , Loss: 1.2935\n",
            "Epoch : 99 , Step : 50 , Loss: 1.2545\n",
            "Epoch : 99 , Step : 55 , Loss: 1.2897\n",
            "Epoch : 99 , Step : 60 , Loss: 1.1828\n",
            "Epoch : 100 , Step : 5 , Loss: 1.2315\n",
            "Epoch : 100 , Step : 10 , Loss: 1.2027\n",
            "Epoch : 100 , Step : 15 , Loss: 1.1749\n",
            "Epoch : 100 , Step : 20 , Loss: 1.1382\n",
            "Epoch : 100 , Step : 25 , Loss: 1.1971\n",
            "Epoch : 100 , Step : 30 , Loss: 1.2426\n",
            "Epoch : 100 , Step : 35 , Loss: 1.2311\n",
            "Epoch : 100 , Step : 40 , Loss: 1.2752\n",
            "Epoch : 100 , Step : 45 , Loss: 1.1371\n",
            "Epoch : 100 , Step : 50 , Loss: 1.2684\n",
            "Epoch : 100 , Step : 55 , Loss: 1.1061\n",
            "Epoch : 100 , Step : 60 , Loss: 1.2083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0_N', '2_UDH', '3_FEA')\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(6)]\n",
        "    n_class_samples = [0 for i in range(6)]\n",
        "    for images, labels in dataloaders['test']:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model1(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(3):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "id": "hevIZ8NzdiHw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "9a63043a-a702-4d24-b743-c3adf7eded0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d89ca6ca4b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mn_class_correct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mn_class_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Net(model)\n",
        "model2.load_state_dict(torch.load('/content/drive/MyDrive/save_train_simclr/epoch-53.pth'))\n",
        "model2.eval()\n",
        "model2 = model2.to(device)\n"
      ],
      "metadata": {
        "id": "xX9TBkFpGPEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0_N', '2_UDH', '3_FEA')\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(3)]\n",
        "    n_class_samples = [0 for i in range(3)]\n",
        "    for images, labels in dataloaders['test']:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model2(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(3):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "id": "kygXvwSVdiKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3e6832-87a7-4b8f-8421-7ff58663929d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 73.33333333333333 %\n",
            "Accuracy of 0_N: 80.0 %\n",
            "Accuracy of 2_UDH: 40.0 %\n",
            "Accuracy of 3_FEA: 100.0 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QihGQPq-diMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GWPGxpndiO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yREgOskKdiTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhpHgkEvbgJp",
        "outputId": "94789116-0a28-4908-98c3-da0950d35c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.1+cu116)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm) (4.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (3.9.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.11.1 timm-0.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "model = timm.create_model('resnet34')‏"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "EoESvxL2b760",
        "outputId": "a28bca6b-5535-45d4-e636-905e04f000fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-724a52b8e920>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model = timm.create_model('resnet34')‏\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "\n",
        "model = timm.create_model('resnet34')\n",
        "‏"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "Fxyc26dgcfAt",
        "outputId": "88078ee9-d56d-439c-d296-a0c1126c8556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-724a52b8e920>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model = timm.create_model('resnet34')‏\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "\n",
        "model = timm.create_model('resnet34')\n",
        "x     = torch.randn(1, 3, 224, 224)\n",
        "model(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ59FCILco0o",
        "outputId": "a0d9d964-c58d-48f9-b543-d20f4c10cd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'epoch': 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            }, '/content/saving12')"
      ],
      "metadata": {
        "id": "UcgD3wuMc1P9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}